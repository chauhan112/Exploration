{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e936c7-0686-43a3-acf1-d5e7334e7277",
   "metadata": {
    "execution": {
     "execution_failed": "2024-09-13T10:27:00.815Z",
     "iopub.execute_input": "2024-09-13T10:26:30.604485Z",
     "iopub.status.busy": "2024-09-13T10:26:30.603486Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c6d1d5f5ccd4679ba476ce58bf1f164",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa4c02ba4e174e70bbcb68552ed5c981",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from diffusers import FluxPipeline\n",
    "\n",
    "pipe = FluxPipeline.from_pretrained(\"black-forest-labs/FLUX.1-schnell\", torch_dtype=torch.bfloat16)\n",
    "pipe.enable_sequential_cpu_offload() #save some VRAM by offloading the model to CPU. Remove this if you have enough GPU power\n",
    "# pipe.\n",
    "prompt = \"portrait of abstraction architects\"\n",
    "image = pipe(\n",
    "    prompt,\n",
    "    guidance_scale=0.0,\n",
    "    num_inference_steps=4,\n",
    "    max_sequence_length=256,width=512, height=512,\n",
    "    generator=torch.Generator(\"cpu\").manual_seed(0)\n",
    ").images[0]\n",
    "image.save(\"flux-schnell-aa.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a01f4770-c9df-4aee-9ff8-41c318878158",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-08T09:59:18.222525Z",
     "iopub.status.busy": "2024-09-08T09:59:18.221523Z",
     "iopub.status.idle": "2024-09-08T09:59:35.546539Z",
     "shell.execute_reply": "2024-09-08T09:59:35.546539Z",
     "shell.execute_reply.started": "2024-09-08T09:59:18.222525Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "974df30c2eb5405eb17ad2d954fd88ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0464ee99100e492bb1e6abe1328a43d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from diffusers import FluxPipeline\n",
    "\n",
    "pipe = FluxPipeline.from_pretrained(\"black-forest-labs/FLUX.1-schnell\", torch_dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d235e51d-21ae-4134-9f09-ce6dd12cd102",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-08T09:56:48.526363Z",
     "iopub.status.busy": "2024-09-08T09:56:48.526363Z",
     "iopub.status.idle": "2024-09-08T09:56:48.530217Z",
     "shell.execute_reply": "2024-09-08T09:56:48.530217Z",
     "shell.execute_reply.started": "2024-09-08T09:56:48.526363Z"
    }
   },
   "outputs": [],
   "source": [
    "list(pipe.transformer.named_children())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "967c0564-4128-4f80-82a0-bd3b5734ae1d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-08T10:31:40.968079Z",
     "iopub.status.busy": "2024-09-08T10:31:40.966561Z",
     "iopub.status.idle": "2024-09-08T10:31:40.972856Z",
     "shell.execute_reply": "2024-09-08T10:31:40.972856Z",
     "shell.execute_reply.started": "2024-09-08T10:31:40.968079Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['_internal_dict', 'vae', 'text_encoder', 'tokenizer', 'unet', 'scheduler', 'safety_checker', 'feature_extractor', 'image_encoder', 'vae_scale_factor', 'image_processor', '_guidance_scale', '_guidance_rescale', '_clip_skip', '_cross_attention_kwargs', '_interrupt', '_num_timesteps', '_progress_bar_config'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.__dict__.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b641badb-422f-431e-a3c1-1c800ebc420f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-08T10:39:58.082639Z",
     "iopub.status.busy": "2024-09-08T10:39:58.082639Z",
     "iopub.status.idle": "2024-09-08T10:39:58.087541Z",
     "shell.execute_reply": "2024-09-08T10:39:58.087029Z",
     "shell.execute_reply.started": "2024-09-08T10:39:58.082639Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_all_modules_name(pipe):\n",
    "    res = []\n",
    "    for k in pipe.__dict__:\n",
    "        if isinstance(getattr(pipe, k), torch.nn.modules.module.Module):\n",
    "            res.append(k)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1b52f8fa-a6c9-4c9f-8bbb-114767fb86cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-08T10:40:12.724778Z",
     "iopub.status.busy": "2024-09-08T10:40:12.724778Z",
     "iopub.status.idle": "2024-09-08T10:40:12.730330Z",
     "shell.execute_reply": "2024-09-08T10:40:12.729796Z",
     "shell.execute_reply.started": "2024-09-08T10:40:12.724778Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vae', 'text_encoder', 'unet', 'safety_checker']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_all_modules_name(pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b9dfeede-c1b9-47fb-8856-cbb5b228fcad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-08T10:41:16.016622Z",
     "iopub.status.busy": "2024-09-08T10:41:16.016622Z",
     "iopub.status.idle": "2024-09-08T10:41:16.023750Z",
     "shell.execute_reply": "2024-09-08T10:41:16.022708Z",
     "shell.execute_reply.started": "2024-09-08T10:41:16.016622Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VaeImageProcessor {\n",
       "  \"_class_name\": \"VaeImageProcessor\",\n",
       "  \"_diffusers_version\": \"0.30.0\",\n",
       "  \"do_binarize\": false,\n",
       "  \"do_convert_grayscale\": false,\n",
       "  \"do_convert_rgb\": false,\n",
       "  \"do_normalize\": true,\n",
       "  \"do_resize\": true,\n",
       "  \"resample\": \"lanczos\",\n",
       "  \"vae_latent_channels\": 4,\n",
       "  \"vae_scale_factor\": 8\n",
       "}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.image_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe3bf830-e06b-44ef-9c22-0ca69d1cbe3b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-13T10:24:53.526874Z",
     "iopub.status.busy": "2024-09-13T10:24:53.526874Z",
     "iopub.status.idle": "2024-09-13T10:24:53.674627Z",
     "shell.execute_reply": "2024-09-13T10:24:53.673621Z",
     "shell.execute_reply.started": "2024-09-13T10:24:53.526874Z"
    }
   },
   "outputs": [],
   "source": [
    "!start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0c1585-7958-48c1-a488-556c8da53efa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bdeb83-313c-4424-9d66-1c23d9a5bca4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f2134a-c707-4551-ab75-750eb7952eea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37aa5e68-12c0-4015-8fa7-b53f62a2484d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805cc8fb-ce85-4c5d-9ce1-ada4df7865dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d61490-b345-46c5-b282-ff7ede8db693",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2299a84c-44d8-4250-aed6-1fb799fdfeeb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-08T10:43:24.292033Z",
     "iopub.status.busy": "2024-09-08T10:43:24.291034Z",
     "iopub.status.idle": "2024-09-08T10:43:24.295538Z",
     "shell.execute_reply": "2024-09-08T10:43:24.295538Z",
     "shell.execute_reply.started": "2024-09-08T10:43:24.292033Z"
    }
   },
   "source": [
    "## a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4c9518-074a-48ea-a223-d0092bbbcc20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40b8ee9-0a00-45b5-a224-3511d5542cbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7072fa-0158-43a2-8596-584716d88657",
   "metadata": {
    "execution": {
     "execution_failed": "2024-09-08T09:59:03.787Z",
     "iopub.execute_input": "2024-09-08T09:58:27.502314Z",
     "iopub.status.busy": "2024-09-08T09:58:27.502314Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submodule sizes before quantization:\n",
      "transformer_blocks: 12315.36 MB\n",
      "single_transformer_blocks: 10262.47 MB\n",
      "transformer_blocks.0: 648.18 MB\n",
      "transformer_blocks.1: 648.18 MB\n",
      "transformer_blocks.2: 648.18 MB\n",
      "transformer_blocks.3: 648.18 MB\n",
      "transformer_blocks.4: 648.18 MB\n",
      "transformer_blocks.5: 648.18 MB\n",
      "transformer_blocks.6: 648.18 MB\n",
      "transformer_blocks.7: 648.18 MB\n",
      "transformer_blocks.8: 648.18 MB\n",
      "transformer_blocks.9: 648.18 MB\n",
      "transformer_blocks.10: 648.18 MB\n",
      "transformer_blocks.11: 648.18 MB\n",
      "transformer_blocks.12: 648.18 MB\n",
      "transformer_blocks.13: 648.18 MB\n",
      "transformer_blocks.14: 648.18 MB\n",
      "transformer_blocks.15: 648.18 MB\n",
      "transformer_blocks.16: 648.18 MB\n",
      "transformer_blocks.17: 648.18 MB\n",
      "transformer_blocks.18: 648.18 MB\n",
      "single_transformer_blocks.0: 270.06 MB\n",
      "single_transformer_blocks.1: 270.06 MB\n",
      "single_transformer_blocks.2: 270.06 MB\n",
      "single_transformer_blocks.3: 270.06 MB\n",
      "single_transformer_blocks.4: 270.06 MB\n",
      "single_transformer_blocks.5: 270.06 MB\n",
      "single_transformer_blocks.6: 270.06 MB\n",
      "single_transformer_blocks.7: 270.06 MB\n",
      "single_transformer_blocks.8: 270.06 MB\n",
      "single_transformer_blocks.9: 270.06 MB\n",
      "single_transformer_blocks.10: 270.06 MB\n",
      "single_transformer_blocks.11: 270.06 MB\n",
      "single_transformer_blocks.12: 270.06 MB\n",
      "single_transformer_blocks.13: 270.06 MB\n",
      "single_transformer_blocks.14: 270.06 MB\n",
      "single_transformer_blocks.15: 270.06 MB\n",
      "single_transformer_blocks.16: 270.06 MB\n",
      "single_transformer_blocks.17: 270.06 MB\n",
      "single_transformer_blocks.18: 270.06 MB\n",
      "single_transformer_blocks.19: 270.06 MB\n",
      "single_transformer_blocks.20: 270.06 MB\n",
      "single_transformer_blocks.21: 270.06 MB\n",
      "single_transformer_blocks.22: 270.06 MB\n",
      "single_transformer_blocks.23: 270.06 MB\n",
      "single_transformer_blocks.24: 270.06 MB\n",
      "single_transformer_blocks.25: 270.06 MB\n",
      "single_transformer_blocks.26: 270.06 MB\n",
      "single_transformer_blocks.27: 270.06 MB\n",
      "single_transformer_blocks.28: 270.06 MB\n",
      "single_transformer_blocks.29: 270.06 MB\n",
      "single_transformer_blocks.30: 270.06 MB\n",
      "single_transformer_blocks.31: 270.06 MB\n",
      "single_transformer_blocks.32: 270.06 MB\n",
      "single_transformer_blocks.33: 270.06 MB\n",
      "single_transformer_blocks.34: 270.06 MB\n",
      "single_transformer_blocks.35: 270.06 MB\n",
      "single_transformer_blocks.36: 270.06 MB\n",
      "single_transformer_blocks.37: 270.06 MB\n",
      "transformer_blocks.0.attn: 144.05 MB\n",
      "transformer_blocks.1.attn: 144.05 MB\n",
      "transformer_blocks.2.attn: 144.05 MB\n",
      "transformer_blocks.3.attn: 144.05 MB\n",
      "transformer_blocks.4.attn: 144.05 MB\n",
      "transformer_blocks.5.attn: 144.05 MB\n",
      "transformer_blocks.6.attn: 144.05 MB\n",
      "transformer_blocks.7.attn: 144.05 MB\n",
      "transformer_blocks.8.attn: 144.05 MB\n",
      "transformer_blocks.9.attn: 144.05 MB\n",
      "transformer_blocks.10.attn: 144.05 MB\n",
      "transformer_blocks.11.attn: 144.05 MB\n",
      "transformer_blocks.12.attn: 144.05 MB\n",
      "transformer_blocks.13.attn: 144.05 MB\n",
      "transformer_blocks.14.attn: 144.05 MB\n",
      "transformer_blocks.15.attn: 144.05 MB\n",
      "transformer_blocks.16.attn: 144.05 MB\n",
      "transformer_blocks.17.attn: 144.05 MB\n",
      "transformer_blocks.18.attn: 144.05 MB\n",
      "transformer_blocks.0.ff: 144.03 MB\n",
      "transformer_blocks.0.ff.net: 144.03 MB\n",
      "transformer_blocks.0.ff_context: 144.03 MB\n",
      "transformer_blocks.0.ff_context.net: 144.03 MB\n",
      "transformer_blocks.1.ff: 144.03 MB\n",
      "transformer_blocks.1.ff.net: 144.03 MB\n",
      "transformer_blocks.1.ff_context: 144.03 MB\n",
      "transformer_blocks.1.ff_context.net: 144.03 MB\n",
      "transformer_blocks.2.ff: 144.03 MB\n",
      "transformer_blocks.2.ff.net: 144.03 MB\n",
      "transformer_blocks.2.ff_context: 144.03 MB\n",
      "transformer_blocks.2.ff_context.net: 144.03 MB\n",
      "transformer_blocks.3.ff: 144.03 MB\n",
      "transformer_blocks.3.ff.net: 144.03 MB\n",
      "transformer_blocks.3.ff_context: 144.03 MB\n",
      "transformer_blocks.3.ff_context.net: 144.03 MB\n",
      "transformer_blocks.4.ff: 144.03 MB\n",
      "transformer_blocks.4.ff.net: 144.03 MB\n",
      "transformer_blocks.4.ff_context: 144.03 MB\n",
      "transformer_blocks.4.ff_context.net: 144.03 MB\n",
      "transformer_blocks.5.ff: 144.03 MB\n",
      "transformer_blocks.5.ff.net: 144.03 MB\n",
      "transformer_blocks.5.ff_context: 144.03 MB\n",
      "transformer_blocks.5.ff_context.net: 144.03 MB\n",
      "transformer_blocks.6.ff: 144.03 MB\n",
      "transformer_blocks.6.ff.net: 144.03 MB\n",
      "transformer_blocks.6.ff_context: 144.03 MB\n",
      "transformer_blocks.6.ff_context.net: 144.03 MB\n",
      "transformer_blocks.7.ff: 144.03 MB\n",
      "transformer_blocks.7.ff.net: 144.03 MB\n",
      "transformer_blocks.7.ff_context: 144.03 MB\n",
      "transformer_blocks.7.ff_context.net: 144.03 MB\n",
      "transformer_blocks.8.ff: 144.03 MB\n",
      "transformer_blocks.8.ff.net: 144.03 MB\n",
      "transformer_blocks.8.ff_context: 144.03 MB\n",
      "transformer_blocks.8.ff_context.net: 144.03 MB\n",
      "transformer_blocks.9.ff: 144.03 MB\n",
      "transformer_blocks.9.ff.net: 144.03 MB\n",
      "transformer_blocks.9.ff_context: 144.03 MB\n",
      "transformer_blocks.9.ff_context.net: 144.03 MB\n",
      "transformer_blocks.10.ff: 144.03 MB\n",
      "transformer_blocks.10.ff.net: 144.03 MB\n",
      "transformer_blocks.10.ff_context: 144.03 MB\n",
      "transformer_blocks.10.ff_context.net: 144.03 MB\n",
      "transformer_blocks.11.ff: 144.03 MB\n",
      "transformer_blocks.11.ff.net: 144.03 MB\n",
      "transformer_blocks.11.ff_context: 144.03 MB\n",
      "transformer_blocks.11.ff_context.net: 144.03 MB\n",
      "transformer_blocks.12.ff: 144.03 MB\n",
      "transformer_blocks.12.ff.net: 144.03 MB\n",
      "transformer_blocks.12.ff_context: 144.03 MB\n",
      "transformer_blocks.12.ff_context.net: 144.03 MB\n",
      "transformer_blocks.13.ff: 144.03 MB\n",
      "transformer_blocks.13.ff.net: 144.03 MB\n",
      "transformer_blocks.13.ff_context: 144.03 MB\n",
      "transformer_blocks.13.ff_context.net: 144.03 MB\n",
      "transformer_blocks.14.ff: 144.03 MB\n",
      "transformer_blocks.14.ff.net: 144.03 MB\n",
      "transformer_blocks.14.ff_context: 144.03 MB\n",
      "transformer_blocks.14.ff_context.net: 144.03 MB\n",
      "transformer_blocks.15.ff: 144.03 MB\n",
      "transformer_blocks.15.ff.net: 144.03 MB\n",
      "transformer_blocks.15.ff_context: 144.03 MB\n",
      "transformer_blocks.15.ff_context.net: 144.03 MB\n",
      "transformer_blocks.16.ff: 144.03 MB\n",
      "transformer_blocks.16.ff.net: 144.03 MB\n",
      "transformer_blocks.16.ff_context: 144.03 MB\n",
      "transformer_blocks.16.ff_context.net: 144.03 MB\n",
      "transformer_blocks.17.ff: 144.03 MB\n",
      "transformer_blocks.17.ff.net: 144.03 MB\n",
      "transformer_blocks.17.ff_context: 144.03 MB\n",
      "transformer_blocks.17.ff_context.net: 144.03 MB\n",
      "transformer_blocks.18.ff: 144.03 MB\n",
      "transformer_blocks.18.ff.net: 144.03 MB\n",
      "transformer_blocks.18.ff_context: 144.03 MB\n",
      "transformer_blocks.18.ff_context.net: 144.03 MB\n",
      "transformer_blocks.0.norm1: 108.04 MB\n",
      "transformer_blocks.0.norm1.linear: 108.04 MB\n",
      "transformer_blocks.0.norm1_context: 108.04 MB\n",
      "transformer_blocks.0.norm1_context.linear: 108.04 MB\n",
      "transformer_blocks.1.norm1: 108.04 MB\n",
      "transformer_blocks.1.norm1.linear: 108.04 MB\n",
      "transformer_blocks.1.norm1_context: 108.04 MB\n",
      "transformer_blocks.1.norm1_context.linear: 108.04 MB\n",
      "transformer_blocks.2.norm1: 108.04 MB\n",
      "transformer_blocks.2.norm1.linear: 108.04 MB\n",
      "transformer_blocks.2.norm1_context: 108.04 MB\n",
      "transformer_blocks.2.norm1_context.linear: 108.04 MB\n",
      "transformer_blocks.3.norm1: 108.04 MB\n",
      "transformer_blocks.3.norm1.linear: 108.04 MB\n",
      "transformer_blocks.3.norm1_context: 108.04 MB\n",
      "transformer_blocks.3.norm1_context.linear: 108.04 MB\n",
      "transformer_blocks.4.norm1: 108.04 MB\n",
      "transformer_blocks.4.norm1.linear: 108.04 MB\n",
      "transformer_blocks.4.norm1_context: 108.04 MB\n",
      "transformer_blocks.4.norm1_context.linear: 108.04 MB\n",
      "transformer_blocks.5.norm1: 108.04 MB\n",
      "transformer_blocks.5.norm1.linear: 108.04 MB\n",
      "transformer_blocks.5.norm1_context: 108.04 MB\n",
      "transformer_blocks.5.norm1_context.linear: 108.04 MB\n",
      "transformer_blocks.6.norm1: 108.04 MB\n",
      "transformer_blocks.6.norm1.linear: 108.04 MB\n",
      "transformer_blocks.6.norm1_context: 108.04 MB\n",
      "transformer_blocks.6.norm1_context.linear: 108.04 MB\n",
      "transformer_blocks.7.norm1: 108.04 MB\n",
      "transformer_blocks.7.norm1.linear: 108.04 MB\n",
      "transformer_blocks.7.norm1_context: 108.04 MB\n",
      "transformer_blocks.7.norm1_context.linear: 108.04 MB\n",
      "transformer_blocks.8.norm1: 108.04 MB\n",
      "transformer_blocks.8.norm1.linear: 108.04 MB\n",
      "transformer_blocks.8.norm1_context: 108.04 MB\n",
      "transformer_blocks.8.norm1_context.linear: 108.04 MB\n",
      "transformer_blocks.9.norm1: 108.04 MB\n",
      "transformer_blocks.9.norm1.linear: 108.04 MB\n",
      "transformer_blocks.9.norm1_context: 108.04 MB\n",
      "transformer_blocks.9.norm1_context.linear: 108.04 MB\n",
      "transformer_blocks.10.norm1: 108.04 MB\n",
      "transformer_blocks.10.norm1.linear: 108.04 MB\n",
      "transformer_blocks.10.norm1_context: 108.04 MB\n",
      "transformer_blocks.10.norm1_context.linear: 108.04 MB\n",
      "transformer_blocks.11.norm1: 108.04 MB\n",
      "transformer_blocks.11.norm1.linear: 108.04 MB\n",
      "transformer_blocks.11.norm1_context: 108.04 MB\n",
      "transformer_blocks.11.norm1_context.linear: 108.04 MB\n",
      "transformer_blocks.12.norm1: 108.04 MB\n",
      "transformer_blocks.12.norm1.linear: 108.04 MB\n",
      "transformer_blocks.12.norm1_context: 108.04 MB\n",
      "transformer_blocks.12.norm1_context.linear: 108.04 MB\n",
      "transformer_blocks.13.norm1: 108.04 MB\n",
      "transformer_blocks.13.norm1.linear: 108.04 MB\n",
      "transformer_blocks.13.norm1_context: 108.04 MB\n",
      "transformer_blocks.13.norm1_context.linear: 108.04 MB\n",
      "transformer_blocks.14.norm1: 108.04 MB\n",
      "transformer_blocks.14.norm1.linear: 108.04 MB\n",
      "transformer_blocks.14.norm1_context: 108.04 MB\n",
      "transformer_blocks.14.norm1_context.linear: 108.04 MB\n",
      "transformer_blocks.15.norm1: 108.04 MB\n",
      "transformer_blocks.15.norm1.linear: 108.04 MB\n",
      "transformer_blocks.15.norm1_context: 108.04 MB\n",
      "transformer_blocks.15.norm1_context.linear: 108.04 MB\n",
      "transformer_blocks.16.norm1: 108.04 MB\n",
      "transformer_blocks.16.norm1.linear: 108.04 MB\n",
      "transformer_blocks.16.norm1_context: 108.04 MB\n",
      "transformer_blocks.16.norm1_context.linear: 108.04 MB\n",
      "transformer_blocks.17.norm1: 108.04 MB\n",
      "transformer_blocks.17.norm1.linear: 108.04 MB\n",
      "transformer_blocks.17.norm1_context: 108.04 MB\n",
      "transformer_blocks.17.norm1_context.linear: 108.04 MB\n",
      "transformer_blocks.18.norm1: 108.04 MB\n",
      "transformer_blocks.18.norm1.linear: 108.04 MB\n",
      "transformer_blocks.18.norm1_context: 108.04 MB\n",
      "transformer_blocks.18.norm1_context.linear: 108.04 MB\n",
      "single_transformer_blocks.0.proj_out: 90.01 MB\n",
      "single_transformer_blocks.1.proj_out: 90.01 MB\n",
      "single_transformer_blocks.2.proj_out: 90.01 MB\n",
      "single_transformer_blocks.3.proj_out: 90.01 MB\n",
      "single_transformer_blocks.4.proj_out: 90.01 MB\n",
      "single_transformer_blocks.5.proj_out: 90.01 MB\n",
      "single_transformer_blocks.6.proj_out: 90.01 MB\n",
      "single_transformer_blocks.7.proj_out: 90.01 MB\n",
      "single_transformer_blocks.8.proj_out: 90.01 MB\n",
      "single_transformer_blocks.9.proj_out: 90.01 MB\n",
      "single_transformer_blocks.10.proj_out: 90.01 MB\n",
      "single_transformer_blocks.11.proj_out: 90.01 MB\n",
      "single_transformer_blocks.12.proj_out: 90.01 MB\n",
      "single_transformer_blocks.13.proj_out: 90.01 MB\n",
      "single_transformer_blocks.14.proj_out: 90.01 MB\n",
      "single_transformer_blocks.15.proj_out: 90.01 MB\n",
      "single_transformer_blocks.16.proj_out: 90.01 MB\n",
      "single_transformer_blocks.17.proj_out: 90.01 MB\n",
      "single_transformer_blocks.18.proj_out: 90.01 MB\n",
      "single_transformer_blocks.19.proj_out: 90.01 MB\n",
      "single_transformer_blocks.20.proj_out: 90.01 MB\n",
      "single_transformer_blocks.21.proj_out: 90.01 MB\n",
      "single_transformer_blocks.22.proj_out: 90.01 MB\n",
      "single_transformer_blocks.23.proj_out: 90.01 MB\n",
      "single_transformer_blocks.24.proj_out: 90.01 MB\n",
      "single_transformer_blocks.25.proj_out: 90.01 MB\n",
      "single_transformer_blocks.26.proj_out: 90.01 MB\n",
      "single_transformer_blocks.27.proj_out: 90.01 MB\n",
      "single_transformer_blocks.28.proj_out: 90.01 MB\n",
      "single_transformer_blocks.29.proj_out: 90.01 MB\n",
      "single_transformer_blocks.30.proj_out: 90.01 MB\n",
      "single_transformer_blocks.31.proj_out: 90.01 MB\n",
      "single_transformer_blocks.32.proj_out: 90.01 MB\n",
      "single_transformer_blocks.33.proj_out: 90.01 MB\n",
      "single_transformer_blocks.34.proj_out: 90.01 MB\n",
      "single_transformer_blocks.35.proj_out: 90.01 MB\n",
      "single_transformer_blocks.36.proj_out: 90.01 MB\n",
      "single_transformer_blocks.37.proj_out: 90.01 MB\n",
      "transformer_blocks.0.ff.net.0: 72.02 MB\n",
      "transformer_blocks.0.ff.net.0.proj: 72.02 MB\n",
      "transformer_blocks.0.ff_context.net.0: 72.02 MB\n",
      "transformer_blocks.0.ff_context.net.0.proj: 72.02 MB\n",
      "transformer_blocks.1.ff.net.0: 72.02 MB\n",
      "transformer_blocks.1.ff.net.0.proj: 72.02 MB\n",
      "transformer_blocks.1.ff_context.net.0: 72.02 MB\n",
      "transformer_blocks.1.ff_context.net.0.proj: 72.02 MB\n",
      "transformer_blocks.2.ff.net.0: 72.02 MB\n",
      "transformer_blocks.2.ff.net.0.proj: 72.02 MB\n",
      "transformer_blocks.2.ff_context.net.0: 72.02 MB\n",
      "transformer_blocks.2.ff_context.net.0.proj: 72.02 MB\n",
      "transformer_blocks.3.ff.net.0: 72.02 MB\n",
      "transformer_blocks.3.ff.net.0.proj: 72.02 MB\n",
      "transformer_blocks.3.ff_context.net.0: 72.02 MB\n",
      "transformer_blocks.3.ff_context.net.0.proj: 72.02 MB\n",
      "transformer_blocks.4.ff.net.0: 72.02 MB\n",
      "transformer_blocks.4.ff.net.0.proj: 72.02 MB\n",
      "transformer_blocks.4.ff_context.net.0: 72.02 MB\n",
      "transformer_blocks.4.ff_context.net.0.proj: 72.02 MB\n",
      "transformer_blocks.5.ff.net.0: 72.02 MB\n",
      "transformer_blocks.5.ff.net.0.proj: 72.02 MB\n",
      "transformer_blocks.5.ff_context.net.0: 72.02 MB\n",
      "transformer_blocks.5.ff_context.net.0.proj: 72.02 MB\n",
      "transformer_blocks.6.ff.net.0: 72.02 MB\n",
      "transformer_blocks.6.ff.net.0.proj: 72.02 MB\n",
      "transformer_blocks.6.ff_context.net.0: 72.02 MB\n",
      "transformer_blocks.6.ff_context.net.0.proj: 72.02 MB\n",
      "transformer_blocks.7.ff.net.0: 72.02 MB\n",
      "transformer_blocks.7.ff.net.0.proj: 72.02 MB\n",
      "transformer_blocks.7.ff_context.net.0: 72.02 MB\n",
      "transformer_blocks.7.ff_context.net.0.proj: 72.02 MB\n",
      "transformer_blocks.8.ff.net.0: 72.02 MB\n",
      "transformer_blocks.8.ff.net.0.proj: 72.02 MB\n",
      "transformer_blocks.8.ff_context.net.0: 72.02 MB\n",
      "transformer_blocks.8.ff_context.net.0.proj: 72.02 MB\n",
      "transformer_blocks.9.ff.net.0: 72.02 MB\n",
      "transformer_blocks.9.ff.net.0.proj: 72.02 MB\n",
      "transformer_blocks.9.ff_context.net.0: 72.02 MB\n",
      "transformer_blocks.9.ff_context.net.0.proj: 72.02 MB\n",
      "transformer_blocks.10.ff.net.0: 72.02 MB\n",
      "transformer_blocks.10.ff.net.0.proj: 72.02 MB\n",
      "transformer_blocks.10.ff_context.net.0: 72.02 MB\n",
      "transformer_blocks.10.ff_context.net.0.proj: 72.02 MB\n",
      "transformer_blocks.11.ff.net.0: 72.02 MB\n",
      "transformer_blocks.11.ff.net.0.proj: 72.02 MB\n",
      "transformer_blocks.11.ff_context.net.0: 72.02 MB\n",
      "transformer_blocks.11.ff_context.net.0.proj: 72.02 MB\n",
      "transformer_blocks.12.ff.net.0: 72.02 MB\n",
      "transformer_blocks.12.ff.net.0.proj: 72.02 MB\n",
      "transformer_blocks.12.ff_context.net.0: 72.02 MB\n",
      "transformer_blocks.12.ff_context.net.0.proj: 72.02 MB\n",
      "transformer_blocks.13.ff.net.0: 72.02 MB\n",
      "transformer_blocks.13.ff.net.0.proj: 72.02 MB\n",
      "transformer_blocks.13.ff_context.net.0: 72.02 MB\n",
      "transformer_blocks.13.ff_context.net.0.proj: 72.02 MB\n",
      "transformer_blocks.14.ff.net.0: 72.02 MB\n",
      "transformer_blocks.14.ff.net.0.proj: 72.02 MB\n",
      "transformer_blocks.14.ff_context.net.0: 72.02 MB\n",
      "transformer_blocks.14.ff_context.net.0.proj: 72.02 MB\n",
      "transformer_blocks.15.ff.net.0: 72.02 MB\n",
      "transformer_blocks.15.ff.net.0.proj: 72.02 MB\n",
      "transformer_blocks.15.ff_context.net.0: 72.02 MB\n",
      "transformer_blocks.15.ff_context.net.0.proj: 72.02 MB\n",
      "transformer_blocks.16.ff.net.0: 72.02 MB\n",
      "transformer_blocks.16.ff.net.0.proj: 72.02 MB\n",
      "transformer_blocks.16.ff_context.net.0: 72.02 MB\n",
      "transformer_blocks.16.ff_context.net.0.proj: 72.02 MB\n",
      "transformer_blocks.17.ff.net.0: 72.02 MB\n",
      "transformer_blocks.17.ff.net.0.proj: 72.02 MB\n",
      "transformer_blocks.17.ff_context.net.0: 72.02 MB\n",
      "transformer_blocks.17.ff_context.net.0.proj: 72.02 MB\n",
      "transformer_blocks.18.ff.net.0: 72.02 MB\n",
      "transformer_blocks.18.ff.net.0.proj: 72.02 MB\n",
      "transformer_blocks.18.ff_context.net.0: 72.02 MB\n",
      "transformer_blocks.18.ff_context.net.0.proj: 72.02 MB\n",
      "single_transformer_blocks.0.proj_mlp: 72.02 MB\n",
      "single_transformer_blocks.1.proj_mlp: 72.02 MB\n",
      "single_transformer_blocks.2.proj_mlp: 72.02 MB\n",
      "single_transformer_blocks.3.proj_mlp: 72.02 MB\n",
      "single_transformer_blocks.4.proj_mlp: 72.02 MB\n",
      "single_transformer_blocks.5.proj_mlp: 72.02 MB\n",
      "single_transformer_blocks.6.proj_mlp: 72.02 MB\n",
      "single_transformer_blocks.7.proj_mlp: 72.02 MB\n",
      "single_transformer_blocks.8.proj_mlp: 72.02 MB\n",
      "single_transformer_blocks.9.proj_mlp: 72.02 MB\n",
      "single_transformer_blocks.10.proj_mlp: 72.02 MB\n",
      "single_transformer_blocks.11.proj_mlp: 72.02 MB\n",
      "single_transformer_blocks.12.proj_mlp: 72.02 MB\n",
      "single_transformer_blocks.13.proj_mlp: 72.02 MB\n",
      "single_transformer_blocks.14.proj_mlp: 72.02 MB\n",
      "single_transformer_blocks.15.proj_mlp: 72.02 MB\n",
      "single_transformer_blocks.16.proj_mlp: 72.02 MB\n",
      "single_transformer_blocks.17.proj_mlp: 72.02 MB\n",
      "single_transformer_blocks.18.proj_mlp: 72.02 MB\n",
      "single_transformer_blocks.19.proj_mlp: 72.02 MB\n",
      "single_transformer_blocks.20.proj_mlp: 72.02 MB\n",
      "single_transformer_blocks.21.proj_mlp: 72.02 MB\n",
      "single_transformer_blocks.22.proj_mlp: 72.02 MB\n",
      "single_transformer_blocks.23.proj_mlp: 72.02 MB\n",
      "single_transformer_blocks.24.proj_mlp: 72.02 MB\n",
      "single_transformer_blocks.25.proj_mlp: 72.02 MB\n",
      "single_transformer_blocks.26.proj_mlp: 72.02 MB\n",
      "single_transformer_blocks.27.proj_mlp: 72.02 MB\n",
      "single_transformer_blocks.28.proj_mlp: 72.02 MB\n",
      "single_transformer_blocks.29.proj_mlp: 72.02 MB\n",
      "single_transformer_blocks.30.proj_mlp: 72.02 MB\n",
      "single_transformer_blocks.31.proj_mlp: 72.02 MB\n",
      "single_transformer_blocks.32.proj_mlp: 72.02 MB\n",
      "single_transformer_blocks.33.proj_mlp: 72.02 MB\n",
      "single_transformer_blocks.34.proj_mlp: 72.02 MB\n",
      "single_transformer_blocks.35.proj_mlp: 72.02 MB\n",
      "single_transformer_blocks.36.proj_mlp: 72.02 MB\n",
      "single_transformer_blocks.37.proj_mlp: 72.02 MB\n",
      "transformer_blocks.0.ff.net.2: 72.01 MB\n",
      "transformer_blocks.0.ff_context.net.2: 72.01 MB\n",
      "transformer_blocks.1.ff.net.2: 72.01 MB\n",
      "transformer_blocks.1.ff_context.net.2: 72.01 MB\n",
      "transformer_blocks.2.ff.net.2: 72.01 MB\n",
      "transformer_blocks.2.ff_context.net.2: 72.01 MB\n",
      "transformer_blocks.3.ff.net.2: 72.01 MB\n",
      "transformer_blocks.3.ff_context.net.2: 72.01 MB\n",
      "transformer_blocks.4.ff.net.2: 72.01 MB\n",
      "transformer_blocks.4.ff_context.net.2: 72.01 MB\n",
      "transformer_blocks.5.ff.net.2: 72.01 MB\n",
      "transformer_blocks.5.ff_context.net.2: 72.01 MB\n",
      "transformer_blocks.6.ff.net.2: 72.01 MB\n",
      "transformer_blocks.6.ff_context.net.2: 72.01 MB\n",
      "transformer_blocks.7.ff.net.2: 72.01 MB\n",
      "transformer_blocks.7.ff_context.net.2: 72.01 MB\n",
      "transformer_blocks.8.ff.net.2: 72.01 MB\n",
      "transformer_blocks.8.ff_context.net.2: 72.01 MB\n",
      "transformer_blocks.9.ff.net.2: 72.01 MB\n",
      "transformer_blocks.9.ff_context.net.2: 72.01 MB\n",
      "transformer_blocks.10.ff.net.2: 72.01 MB\n",
      "transformer_blocks.10.ff_context.net.2: 72.01 MB\n",
      "transformer_blocks.11.ff.net.2: 72.01 MB\n",
      "transformer_blocks.11.ff_context.net.2: 72.01 MB\n",
      "transformer_blocks.12.ff.net.2: 72.01 MB\n",
      "transformer_blocks.12.ff_context.net.2: 72.01 MB\n",
      "transformer_blocks.13.ff.net.2: 72.01 MB\n",
      "transformer_blocks.13.ff_context.net.2: 72.01 MB\n",
      "transformer_blocks.14.ff.net.2: 72.01 MB\n",
      "transformer_blocks.14.ff_context.net.2: 72.01 MB\n",
      "transformer_blocks.15.ff.net.2: 72.01 MB\n",
      "transformer_blocks.15.ff_context.net.2: 72.01 MB\n",
      "transformer_blocks.16.ff.net.2: 72.01 MB\n",
      "transformer_blocks.16.ff_context.net.2: 72.01 MB\n",
      "transformer_blocks.17.ff.net.2: 72.01 MB\n",
      "transformer_blocks.17.ff_context.net.2: 72.01 MB\n",
      "transformer_blocks.18.ff.net.2: 72.01 MB\n",
      "transformer_blocks.18.ff_context.net.2: 72.01 MB\n",
      "single_transformer_blocks.0.attn: 54.02 MB\n",
      "single_transformer_blocks.1.attn: 54.02 MB\n",
      "single_transformer_blocks.2.attn: 54.02 MB\n",
      "single_transformer_blocks.3.attn: 54.02 MB\n",
      "single_transformer_blocks.4.attn: 54.02 MB\n",
      "single_transformer_blocks.5.attn: 54.02 MB\n",
      "single_transformer_blocks.6.attn: 54.02 MB\n",
      "single_transformer_blocks.7.attn: 54.02 MB\n",
      "single_transformer_blocks.8.attn: 54.02 MB\n",
      "single_transformer_blocks.9.attn: 54.02 MB\n",
      "single_transformer_blocks.10.attn: 54.02 MB\n",
      "single_transformer_blocks.11.attn: 54.02 MB\n",
      "single_transformer_blocks.12.attn: 54.02 MB\n",
      "single_transformer_blocks.13.attn: 54.02 MB\n",
      "single_transformer_blocks.14.attn: 54.02 MB\n",
      "single_transformer_blocks.15.attn: 54.02 MB\n",
      "single_transformer_blocks.16.attn: 54.02 MB\n",
      "single_transformer_blocks.17.attn: 54.02 MB\n",
      "single_transformer_blocks.18.attn: 54.02 MB\n",
      "single_transformer_blocks.19.attn: 54.02 MB\n",
      "single_transformer_blocks.20.attn: 54.02 MB\n",
      "single_transformer_blocks.21.attn: 54.02 MB\n",
      "single_transformer_blocks.22.attn: 54.02 MB\n",
      "single_transformer_blocks.23.attn: 54.02 MB\n",
      "single_transformer_blocks.24.attn: 54.02 MB\n",
      "single_transformer_blocks.25.attn: 54.02 MB\n",
      "single_transformer_blocks.26.attn: 54.02 MB\n",
      "single_transformer_blocks.27.attn: 54.02 MB\n",
      "single_transformer_blocks.28.attn: 54.02 MB\n",
      "single_transformer_blocks.29.attn: 54.02 MB\n",
      "single_transformer_blocks.30.attn: 54.02 MB\n",
      "single_transformer_blocks.31.attn: 54.02 MB\n",
      "single_transformer_blocks.32.attn: 54.02 MB\n",
      "single_transformer_blocks.33.attn: 54.02 MB\n",
      "single_transformer_blocks.34.attn: 54.02 MB\n",
      "single_transformer_blocks.35.attn: 54.02 MB\n",
      "single_transformer_blocks.36.attn: 54.02 MB\n",
      "single_transformer_blocks.37.attn: 54.02 MB\n",
      "single_transformer_blocks.0.norm: 54.02 MB\n",
      "single_transformer_blocks.0.norm.linear: 54.02 MB\n",
      "single_transformer_blocks.1.norm: 54.02 MB\n",
      "single_transformer_blocks.1.norm.linear: 54.02 MB\n",
      "single_transformer_blocks.2.norm: 54.02 MB\n",
      "single_transformer_blocks.2.norm.linear: 54.02 MB\n",
      "single_transformer_blocks.3.norm: 54.02 MB\n",
      "single_transformer_blocks.3.norm.linear: 54.02 MB\n",
      "single_transformer_blocks.4.norm: 54.02 MB\n",
      "single_transformer_blocks.4.norm.linear: 54.02 MB\n",
      "single_transformer_blocks.5.norm: 54.02 MB\n",
      "single_transformer_blocks.5.norm.linear: 54.02 MB\n",
      "single_transformer_blocks.6.norm: 54.02 MB\n",
      "single_transformer_blocks.6.norm.linear: 54.02 MB\n",
      "single_transformer_blocks.7.norm: 54.02 MB\n",
      "single_transformer_blocks.7.norm.linear: 54.02 MB\n",
      "single_transformer_blocks.8.norm: 54.02 MB\n",
      "single_transformer_blocks.8.norm.linear: 54.02 MB\n",
      "single_transformer_blocks.9.norm: 54.02 MB\n",
      "single_transformer_blocks.9.norm.linear: 54.02 MB\n",
      "single_transformer_blocks.10.norm: 54.02 MB\n",
      "single_transformer_blocks.10.norm.linear: 54.02 MB\n",
      "single_transformer_blocks.11.norm: 54.02 MB\n",
      "single_transformer_blocks.11.norm.linear: 54.02 MB\n",
      "single_transformer_blocks.12.norm: 54.02 MB\n",
      "single_transformer_blocks.12.norm.linear: 54.02 MB\n",
      "single_transformer_blocks.13.norm: 54.02 MB\n",
      "single_transformer_blocks.13.norm.linear: 54.02 MB\n",
      "single_transformer_blocks.14.norm: 54.02 MB\n",
      "single_transformer_blocks.14.norm.linear: 54.02 MB\n",
      "single_transformer_blocks.15.norm: 54.02 MB\n",
      "single_transformer_blocks.15.norm.linear: 54.02 MB\n",
      "single_transformer_blocks.16.norm: 54.02 MB\n",
      "single_transformer_blocks.16.norm.linear: 54.02 MB\n",
      "single_transformer_blocks.17.norm: 54.02 MB\n",
      "single_transformer_blocks.17.norm.linear: 54.02 MB\n",
      "single_transformer_blocks.18.norm: 54.02 MB\n",
      "single_transformer_blocks.18.norm.linear: 54.02 MB\n",
      "single_transformer_blocks.19.norm: 54.02 MB\n",
      "single_transformer_blocks.19.norm.linear: 54.02 MB\n",
      "single_transformer_blocks.20.norm: 54.02 MB\n",
      "single_transformer_blocks.20.norm.linear: 54.02 MB\n",
      "single_transformer_blocks.21.norm: 54.02 MB\n",
      "single_transformer_blocks.21.norm.linear: 54.02 MB\n",
      "single_transformer_blocks.22.norm: 54.02 MB\n",
      "single_transformer_blocks.22.norm.linear: 54.02 MB\n",
      "single_transformer_blocks.23.norm: 54.02 MB\n",
      "single_transformer_blocks.23.norm.linear: 54.02 MB\n",
      "single_transformer_blocks.24.norm: 54.02 MB\n",
      "single_transformer_blocks.24.norm.linear: 54.02 MB\n",
      "single_transformer_blocks.25.norm: 54.02 MB\n",
      "single_transformer_blocks.25.norm.linear: 54.02 MB\n",
      "single_transformer_blocks.26.norm: 54.02 MB\n",
      "single_transformer_blocks.26.norm.linear: 54.02 MB\n",
      "single_transformer_blocks.27.norm: 54.02 MB\n",
      "single_transformer_blocks.27.norm.linear: 54.02 MB\n",
      "single_transformer_blocks.28.norm: 54.02 MB\n",
      "single_transformer_blocks.28.norm.linear: 54.02 MB\n",
      "single_transformer_blocks.29.norm: 54.02 MB\n",
      "single_transformer_blocks.29.norm.linear: 54.02 MB\n",
      "single_transformer_blocks.30.norm: 54.02 MB\n",
      "single_transformer_blocks.30.norm.linear: 54.02 MB\n",
      "single_transformer_blocks.31.norm: 54.02 MB\n",
      "single_transformer_blocks.31.norm.linear: 54.02 MB\n",
      "single_transformer_blocks.32.norm: 54.02 MB\n",
      "single_transformer_blocks.32.norm.linear: 54.02 MB\n",
      "single_transformer_blocks.33.norm: 54.02 MB\n",
      "single_transformer_blocks.33.norm.linear: 54.02 MB\n",
      "single_transformer_blocks.34.norm: 54.02 MB\n",
      "single_transformer_blocks.34.norm.linear: 54.02 MB\n",
      "single_transformer_blocks.35.norm: 54.02 MB\n",
      "single_transformer_blocks.35.norm.linear: 54.02 MB\n",
      "single_transformer_blocks.36.norm: 54.02 MB\n",
      "single_transformer_blocks.36.norm.linear: 54.02 MB\n",
      "single_transformer_blocks.37.norm: 54.02 MB\n",
      "single_transformer_blocks.37.norm.linear: 54.02 MB\n",
      "time_text_embed: 42.02 MB\n",
      "norm_out: 36.01 MB\n",
      "norm_out.linear: 36.01 MB\n",
      "context_embedder: 24.01 MB\n",
      "time_text_embed.text_embedder: 22.51 MB\n",
      "time_text_embed.timestep_embedder: 19.51 MB\n",
      "time_text_embed.timestep_embedder.linear_2: 18.01 MB\n",
      "time_text_embed.text_embedder.linear_2: 18.01 MB\n",
      "transformer_blocks.0.attn.to_q: 18.01 MB\n",
      "transformer_blocks.0.attn.to_k: 18.01 MB\n",
      "transformer_blocks.0.attn.to_v: 18.01 MB\n",
      "transformer_blocks.0.attn.add_k_proj: 18.01 MB\n",
      "transformer_blocks.0.attn.add_v_proj: 18.01 MB\n",
      "transformer_blocks.0.attn.add_q_proj: 18.01 MB\n",
      "transformer_blocks.0.attn.to_out: 18.01 MB\n",
      "transformer_blocks.0.attn.to_out.0: 18.01 MB\n",
      "transformer_blocks.0.attn.to_add_out: 18.01 MB\n",
      "transformer_blocks.1.attn.to_q: 18.01 MB\n",
      "transformer_blocks.1.attn.to_k: 18.01 MB\n",
      "transformer_blocks.1.attn.to_v: 18.01 MB\n",
      "transformer_blocks.1.attn.add_k_proj: 18.01 MB\n",
      "transformer_blocks.1.attn.add_v_proj: 18.01 MB\n",
      "transformer_blocks.1.attn.add_q_proj: 18.01 MB\n",
      "transformer_blocks.1.attn.to_out: 18.01 MB\n",
      "transformer_blocks.1.attn.to_out.0: 18.01 MB\n",
      "transformer_blocks.1.attn.to_add_out: 18.01 MB\n",
      "transformer_blocks.2.attn.to_q: 18.01 MB\n",
      "transformer_blocks.2.attn.to_k: 18.01 MB\n",
      "transformer_blocks.2.attn.to_v: 18.01 MB\n",
      "transformer_blocks.2.attn.add_k_proj: 18.01 MB\n",
      "transformer_blocks.2.attn.add_v_proj: 18.01 MB\n",
      "transformer_blocks.2.attn.add_q_proj: 18.01 MB\n",
      "transformer_blocks.2.attn.to_out: 18.01 MB\n",
      "transformer_blocks.2.attn.to_out.0: 18.01 MB\n",
      "transformer_blocks.2.attn.to_add_out: 18.01 MB\n",
      "transformer_blocks.3.attn.to_q: 18.01 MB\n",
      "transformer_blocks.3.attn.to_k: 18.01 MB\n",
      "transformer_blocks.3.attn.to_v: 18.01 MB\n",
      "transformer_blocks.3.attn.add_k_proj: 18.01 MB\n",
      "transformer_blocks.3.attn.add_v_proj: 18.01 MB\n",
      "transformer_blocks.3.attn.add_q_proj: 18.01 MB\n",
      "transformer_blocks.3.attn.to_out: 18.01 MB\n",
      "transformer_blocks.3.attn.to_out.0: 18.01 MB\n",
      "transformer_blocks.3.attn.to_add_out: 18.01 MB\n",
      "transformer_blocks.4.attn.to_q: 18.01 MB\n",
      "transformer_blocks.4.attn.to_k: 18.01 MB\n",
      "transformer_blocks.4.attn.to_v: 18.01 MB\n",
      "transformer_blocks.4.attn.add_k_proj: 18.01 MB\n",
      "transformer_blocks.4.attn.add_v_proj: 18.01 MB\n",
      "transformer_blocks.4.attn.add_q_proj: 18.01 MB\n",
      "transformer_blocks.4.attn.to_out: 18.01 MB\n",
      "transformer_blocks.4.attn.to_out.0: 18.01 MB\n",
      "transformer_blocks.4.attn.to_add_out: 18.01 MB\n",
      "transformer_blocks.5.attn.to_q: 18.01 MB\n",
      "transformer_blocks.5.attn.to_k: 18.01 MB\n",
      "transformer_blocks.5.attn.to_v: 18.01 MB\n",
      "transformer_blocks.5.attn.add_k_proj: 18.01 MB\n",
      "transformer_blocks.5.attn.add_v_proj: 18.01 MB\n",
      "transformer_blocks.5.attn.add_q_proj: 18.01 MB\n",
      "transformer_blocks.5.attn.to_out: 18.01 MB\n",
      "transformer_blocks.5.attn.to_out.0: 18.01 MB\n",
      "transformer_blocks.5.attn.to_add_out: 18.01 MB\n",
      "transformer_blocks.6.attn.to_q: 18.01 MB\n",
      "transformer_blocks.6.attn.to_k: 18.01 MB\n",
      "transformer_blocks.6.attn.to_v: 18.01 MB\n",
      "transformer_blocks.6.attn.add_k_proj: 18.01 MB\n",
      "transformer_blocks.6.attn.add_v_proj: 18.01 MB\n",
      "transformer_blocks.6.attn.add_q_proj: 18.01 MB\n",
      "transformer_blocks.6.attn.to_out: 18.01 MB\n",
      "transformer_blocks.6.attn.to_out.0: 18.01 MB\n",
      "transformer_blocks.6.attn.to_add_out: 18.01 MB\n",
      "transformer_blocks.7.attn.to_q: 18.01 MB\n",
      "transformer_blocks.7.attn.to_k: 18.01 MB\n",
      "transformer_blocks.7.attn.to_v: 18.01 MB\n",
      "transformer_blocks.7.attn.add_k_proj: 18.01 MB\n",
      "transformer_blocks.7.attn.add_v_proj: 18.01 MB\n",
      "transformer_blocks.7.attn.add_q_proj: 18.01 MB\n",
      "transformer_blocks.7.attn.to_out: 18.01 MB\n",
      "transformer_blocks.7.attn.to_out.0: 18.01 MB\n",
      "transformer_blocks.7.attn.to_add_out: 18.01 MB\n",
      "transformer_blocks.8.attn.to_q: 18.01 MB\n",
      "transformer_blocks.8.attn.to_k: 18.01 MB\n",
      "transformer_blocks.8.attn.to_v: 18.01 MB\n",
      "transformer_blocks.8.attn.add_k_proj: 18.01 MB\n",
      "transformer_blocks.8.attn.add_v_proj: 18.01 MB\n",
      "transformer_blocks.8.attn.add_q_proj: 18.01 MB\n",
      "transformer_blocks.8.attn.to_out: 18.01 MB\n",
      "transformer_blocks.8.attn.to_out.0: 18.01 MB\n",
      "transformer_blocks.8.attn.to_add_out: 18.01 MB\n",
      "transformer_blocks.9.attn.to_q: 18.01 MB\n",
      "transformer_blocks.9.attn.to_k: 18.01 MB\n",
      "transformer_blocks.9.attn.to_v: 18.01 MB\n",
      "transformer_blocks.9.attn.add_k_proj: 18.01 MB\n",
      "transformer_blocks.9.attn.add_v_proj: 18.01 MB\n",
      "transformer_blocks.9.attn.add_q_proj: 18.01 MB\n",
      "transformer_blocks.9.attn.to_out: 18.01 MB\n",
      "transformer_blocks.9.attn.to_out.0: 18.01 MB\n",
      "transformer_blocks.9.attn.to_add_out: 18.01 MB\n",
      "transformer_blocks.10.attn.to_q: 18.01 MB\n",
      "transformer_blocks.10.attn.to_k: 18.01 MB\n",
      "transformer_blocks.10.attn.to_v: 18.01 MB\n",
      "transformer_blocks.10.attn.add_k_proj: 18.01 MB\n",
      "transformer_blocks.10.attn.add_v_proj: 18.01 MB\n",
      "transformer_blocks.10.attn.add_q_proj: 18.01 MB\n",
      "transformer_blocks.10.attn.to_out: 18.01 MB\n",
      "transformer_blocks.10.attn.to_out.0: 18.01 MB\n",
      "transformer_blocks.10.attn.to_add_out: 18.01 MB\n",
      "transformer_blocks.11.attn.to_q: 18.01 MB\n",
      "transformer_blocks.11.attn.to_k: 18.01 MB\n",
      "transformer_blocks.11.attn.to_v: 18.01 MB\n",
      "transformer_blocks.11.attn.add_k_proj: 18.01 MB\n",
      "transformer_blocks.11.attn.add_v_proj: 18.01 MB\n",
      "transformer_blocks.11.attn.add_q_proj: 18.01 MB\n",
      "transformer_blocks.11.attn.to_out: 18.01 MB\n",
      "transformer_blocks.11.attn.to_out.0: 18.01 MB\n",
      "transformer_blocks.11.attn.to_add_out: 18.01 MB\n",
      "transformer_blocks.12.attn.to_q: 18.01 MB\n",
      "transformer_blocks.12.attn.to_k: 18.01 MB\n",
      "transformer_blocks.12.attn.to_v: 18.01 MB\n",
      "transformer_blocks.12.attn.add_k_proj: 18.01 MB\n",
      "transformer_blocks.12.attn.add_v_proj: 18.01 MB\n",
      "transformer_blocks.12.attn.add_q_proj: 18.01 MB\n",
      "transformer_blocks.12.attn.to_out: 18.01 MB\n",
      "transformer_blocks.12.attn.to_out.0: 18.01 MB\n",
      "transformer_blocks.12.attn.to_add_out: 18.01 MB\n",
      "transformer_blocks.13.attn.to_q: 18.01 MB\n",
      "transformer_blocks.13.attn.to_k: 18.01 MB\n",
      "transformer_blocks.13.attn.to_v: 18.01 MB\n",
      "transformer_blocks.13.attn.add_k_proj: 18.01 MB\n",
      "transformer_blocks.13.attn.add_v_proj: 18.01 MB\n",
      "transformer_blocks.13.attn.add_q_proj: 18.01 MB\n",
      "transformer_blocks.13.attn.to_out: 18.01 MB\n",
      "transformer_blocks.13.attn.to_out.0: 18.01 MB\n",
      "transformer_blocks.13.attn.to_add_out: 18.01 MB\n",
      "transformer_blocks.14.attn.to_q: 18.01 MB\n",
      "transformer_blocks.14.attn.to_k: 18.01 MB\n",
      "transformer_blocks.14.attn.to_v: 18.01 MB\n",
      "transformer_blocks.14.attn.add_k_proj: 18.01 MB\n",
      "transformer_blocks.14.attn.add_v_proj: 18.01 MB\n",
      "transformer_blocks.14.attn.add_q_proj: 18.01 MB\n",
      "transformer_blocks.14.attn.to_out: 18.01 MB\n",
      "transformer_blocks.14.attn.to_out.0: 18.01 MB\n",
      "transformer_blocks.14.attn.to_add_out: 18.01 MB\n",
      "transformer_blocks.15.attn.to_q: 18.01 MB\n",
      "transformer_blocks.15.attn.to_k: 18.01 MB\n",
      "transformer_blocks.15.attn.to_v: 18.01 MB\n",
      "transformer_blocks.15.attn.add_k_proj: 18.01 MB\n",
      "transformer_blocks.15.attn.add_v_proj: 18.01 MB\n",
      "transformer_blocks.15.attn.add_q_proj: 18.01 MB\n",
      "transformer_blocks.15.attn.to_out: 18.01 MB\n",
      "transformer_blocks.15.attn.to_out.0: 18.01 MB\n",
      "transformer_blocks.15.attn.to_add_out: 18.01 MB\n",
      "transformer_blocks.16.attn.to_q: 18.01 MB\n",
      "transformer_blocks.16.attn.to_k: 18.01 MB\n",
      "transformer_blocks.16.attn.to_v: 18.01 MB\n",
      "transformer_blocks.16.attn.add_k_proj: 18.01 MB\n",
      "transformer_blocks.16.attn.add_v_proj: 18.01 MB\n",
      "transformer_blocks.16.attn.add_q_proj: 18.01 MB\n",
      "transformer_blocks.16.attn.to_out: 18.01 MB\n",
      "transformer_blocks.16.attn.to_out.0: 18.01 MB\n",
      "transformer_blocks.16.attn.to_add_out: 18.01 MB\n",
      "transformer_blocks.17.attn.to_q: 18.01 MB\n",
      "transformer_blocks.17.attn.to_k: 18.01 MB\n",
      "transformer_blocks.17.attn.to_v: 18.01 MB\n",
      "transformer_blocks.17.attn.add_k_proj: 18.01 MB\n",
      "transformer_blocks.17.attn.add_v_proj: 18.01 MB\n",
      "transformer_blocks.17.attn.add_q_proj: 18.01 MB\n",
      "transformer_blocks.17.attn.to_out: 18.01 MB\n",
      "transformer_blocks.17.attn.to_out.0: 18.01 MB\n",
      "transformer_blocks.17.attn.to_add_out: 18.01 MB\n",
      "transformer_blocks.18.attn.to_q: 18.01 MB\n",
      "transformer_blocks.18.attn.to_k: 18.01 MB\n",
      "transformer_blocks.18.attn.to_v: 18.01 MB\n",
      "transformer_blocks.18.attn.add_k_proj: 18.01 MB\n",
      "transformer_blocks.18.attn.add_v_proj: 18.01 MB\n",
      "transformer_blocks.18.attn.add_q_proj: 18.01 MB\n",
      "transformer_blocks.18.attn.to_out: 18.01 MB\n",
      "transformer_blocks.18.attn.to_out.0: 18.01 MB\n",
      "transformer_blocks.18.attn.to_add_out: 18.01 MB\n",
      "single_transformer_blocks.0.attn.to_q: 18.01 MB\n",
      "single_transformer_blocks.0.attn.to_k: 18.01 MB\n",
      "single_transformer_blocks.0.attn.to_v: 18.01 MB\n",
      "single_transformer_blocks.1.attn.to_q: 18.01 MB\n",
      "single_transformer_blocks.1.attn.to_k: 18.01 MB\n",
      "single_transformer_blocks.1.attn.to_v: 18.01 MB\n",
      "single_transformer_blocks.2.attn.to_q: 18.01 MB\n",
      "single_transformer_blocks.2.attn.to_k: 18.01 MB\n",
      "single_transformer_blocks.2.attn.to_v: 18.01 MB\n",
      "single_transformer_blocks.3.attn.to_q: 18.01 MB\n",
      "single_transformer_blocks.3.attn.to_k: 18.01 MB\n",
      "single_transformer_blocks.3.attn.to_v: 18.01 MB\n",
      "single_transformer_blocks.4.attn.to_q: 18.01 MB\n",
      "single_transformer_blocks.4.attn.to_k: 18.01 MB\n",
      "single_transformer_blocks.4.attn.to_v: 18.01 MB\n",
      "single_transformer_blocks.5.attn.to_q: 18.01 MB\n",
      "single_transformer_blocks.5.attn.to_k: 18.01 MB\n",
      "single_transformer_blocks.5.attn.to_v: 18.01 MB\n",
      "single_transformer_blocks.6.attn.to_q: 18.01 MB\n",
      "single_transformer_blocks.6.attn.to_k: 18.01 MB\n",
      "single_transformer_blocks.6.attn.to_v: 18.01 MB\n",
      "single_transformer_blocks.7.attn.to_q: 18.01 MB\n",
      "single_transformer_blocks.7.attn.to_k: 18.01 MB\n",
      "single_transformer_blocks.7.attn.to_v: 18.01 MB\n",
      "single_transformer_blocks.8.attn.to_q: 18.01 MB\n",
      "single_transformer_blocks.8.attn.to_k: 18.01 MB\n",
      "single_transformer_blocks.8.attn.to_v: 18.01 MB\n",
      "single_transformer_blocks.9.attn.to_q: 18.01 MB\n",
      "single_transformer_blocks.9.attn.to_k: 18.01 MB\n",
      "single_transformer_blocks.9.attn.to_v: 18.01 MB\n",
      "single_transformer_blocks.10.attn.to_q: 18.01 MB\n",
      "single_transformer_blocks.10.attn.to_k: 18.01 MB\n",
      "single_transformer_blocks.10.attn.to_v: 18.01 MB\n",
      "single_transformer_blocks.11.attn.to_q: 18.01 MB\n",
      "single_transformer_blocks.11.attn.to_k: 18.01 MB\n",
      "single_transformer_blocks.11.attn.to_v: 18.01 MB\n",
      "single_transformer_blocks.12.attn.to_q: 18.01 MB\n",
      "single_transformer_blocks.12.attn.to_k: 18.01 MB\n",
      "single_transformer_blocks.12.attn.to_v: 18.01 MB\n",
      "single_transformer_blocks.13.attn.to_q: 18.01 MB\n",
      "single_transformer_blocks.13.attn.to_k: 18.01 MB\n",
      "single_transformer_blocks.13.attn.to_v: 18.01 MB\n",
      "single_transformer_blocks.14.attn.to_q: 18.01 MB\n",
      "single_transformer_blocks.14.attn.to_k: 18.01 MB\n",
      "single_transformer_blocks.14.attn.to_v: 18.01 MB\n",
      "single_transformer_blocks.15.attn.to_q: 18.01 MB\n",
      "single_transformer_blocks.15.attn.to_k: 18.01 MB\n",
      "single_transformer_blocks.15.attn.to_v: 18.01 MB\n",
      "single_transformer_blocks.16.attn.to_q: 18.01 MB\n",
      "single_transformer_blocks.16.attn.to_k: 18.01 MB\n",
      "single_transformer_blocks.16.attn.to_v: 18.01 MB\n",
      "single_transformer_blocks.17.attn.to_q: 18.01 MB\n",
      "single_transformer_blocks.17.attn.to_k: 18.01 MB\n",
      "single_transformer_blocks.17.attn.to_v: 18.01 MB\n",
      "single_transformer_blocks.18.attn.to_q: 18.01 MB\n",
      "single_transformer_blocks.18.attn.to_k: 18.01 MB\n",
      "single_transformer_blocks.18.attn.to_v: 18.01 MB\n",
      "single_transformer_blocks.19.attn.to_q: 18.01 MB\n",
      "single_transformer_blocks.19.attn.to_k: 18.01 MB\n",
      "single_transformer_blocks.19.attn.to_v: 18.01 MB\n",
      "single_transformer_blocks.20.attn.to_q: 18.01 MB\n",
      "single_transformer_blocks.20.attn.to_k: 18.01 MB\n",
      "single_transformer_blocks.20.attn.to_v: 18.01 MB\n",
      "single_transformer_blocks.21.attn.to_q: 18.01 MB\n",
      "single_transformer_blocks.21.attn.to_k: 18.01 MB\n",
      "single_transformer_blocks.21.attn.to_v: 18.01 MB\n",
      "single_transformer_blocks.22.attn.to_q: 18.01 MB\n",
      "single_transformer_blocks.22.attn.to_k: 18.01 MB\n",
      "single_transformer_blocks.22.attn.to_v: 18.01 MB\n",
      "single_transformer_blocks.23.attn.to_q: 18.01 MB\n",
      "single_transformer_blocks.23.attn.to_k: 18.01 MB\n",
      "single_transformer_blocks.23.attn.to_v: 18.01 MB\n",
      "single_transformer_blocks.24.attn.to_q: 18.01 MB\n",
      "single_transformer_blocks.24.attn.to_k: 18.01 MB\n",
      "single_transformer_blocks.24.attn.to_v: 18.01 MB\n",
      "single_transformer_blocks.25.attn.to_q: 18.01 MB\n",
      "single_transformer_blocks.25.attn.to_k: 18.01 MB\n",
      "single_transformer_blocks.25.attn.to_v: 18.01 MB\n",
      "single_transformer_blocks.26.attn.to_q: 18.01 MB\n",
      "single_transformer_blocks.26.attn.to_k: 18.01 MB\n",
      "single_transformer_blocks.26.attn.to_v: 18.01 MB\n",
      "single_transformer_blocks.27.attn.to_q: 18.01 MB\n",
      "single_transformer_blocks.27.attn.to_k: 18.01 MB\n",
      "single_transformer_blocks.27.attn.to_v: 18.01 MB\n",
      "single_transformer_blocks.28.attn.to_q: 18.01 MB\n",
      "single_transformer_blocks.28.attn.to_k: 18.01 MB\n",
      "single_transformer_blocks.28.attn.to_v: 18.01 MB\n",
      "single_transformer_blocks.29.attn.to_q: 18.01 MB\n",
      "single_transformer_blocks.29.attn.to_k: 18.01 MB\n",
      "single_transformer_blocks.29.attn.to_v: 18.01 MB\n",
      "single_transformer_blocks.30.attn.to_q: 18.01 MB\n",
      "single_transformer_blocks.30.attn.to_k: 18.01 MB\n",
      "single_transformer_blocks.30.attn.to_v: 18.01 MB\n",
      "single_transformer_blocks.31.attn.to_q: 18.01 MB\n",
      "single_transformer_blocks.31.attn.to_k: 18.01 MB\n",
      "single_transformer_blocks.31.attn.to_v: 18.01 MB\n",
      "single_transformer_blocks.32.attn.to_q: 18.01 MB\n",
      "single_transformer_blocks.32.attn.to_k: 18.01 MB\n",
      "single_transformer_blocks.32.attn.to_v: 18.01 MB\n",
      "single_transformer_blocks.33.attn.to_q: 18.01 MB\n",
      "single_transformer_blocks.33.attn.to_k: 18.01 MB\n",
      "single_transformer_blocks.33.attn.to_v: 18.01 MB\n",
      "single_transformer_blocks.34.attn.to_q: 18.01 MB\n",
      "single_transformer_blocks.34.attn.to_k: 18.01 MB\n",
      "single_transformer_blocks.34.attn.to_v: 18.01 MB\n",
      "single_transformer_blocks.35.attn.to_q: 18.01 MB\n",
      "single_transformer_blocks.35.attn.to_k: 18.01 MB\n",
      "single_transformer_blocks.35.attn.to_v: 18.01 MB\n",
      "single_transformer_blocks.36.attn.to_q: 18.01 MB\n",
      "single_transformer_blocks.36.attn.to_k: 18.01 MB\n",
      "single_transformer_blocks.36.attn.to_v: 18.01 MB\n",
      "single_transformer_blocks.37.attn.to_q: 18.01 MB\n",
      "single_transformer_blocks.37.attn.to_k: 18.01 MB\n",
      "single_transformer_blocks.37.attn.to_v: 18.01 MB\n",
      "time_text_embed.text_embedder.linear_1: 4.51 MB\n",
      "time_text_embed.timestep_embedder.linear_1: 1.51 MB\n",
      "x_embedder: 0.38 MB\n",
      "proj_out: 0.38 MB\n",
      "transformer_blocks.0.attn.norm_q: 0.00 MB\n",
      "transformer_blocks.0.attn.norm_k: 0.00 MB\n",
      "transformer_blocks.0.attn.norm_added_q: 0.00 MB\n",
      "transformer_blocks.0.attn.norm_added_k: 0.00 MB\n",
      "transformer_blocks.1.attn.norm_q: 0.00 MB\n",
      "transformer_blocks.1.attn.norm_k: 0.00 MB\n",
      "transformer_blocks.1.attn.norm_added_q: 0.00 MB\n",
      "transformer_blocks.1.attn.norm_added_k: 0.00 MB\n",
      "transformer_blocks.2.attn.norm_q: 0.00 MB\n",
      "transformer_blocks.2.attn.norm_k: 0.00 MB\n",
      "transformer_blocks.2.attn.norm_added_q: 0.00 MB\n",
      "transformer_blocks.2.attn.norm_added_k: 0.00 MB\n",
      "transformer_blocks.3.attn.norm_q: 0.00 MB\n",
      "transformer_blocks.3.attn.norm_k: 0.00 MB\n",
      "transformer_blocks.3.attn.norm_added_q: 0.00 MB\n",
      "transformer_blocks.3.attn.norm_added_k: 0.00 MB\n",
      "transformer_blocks.4.attn.norm_q: 0.00 MB\n",
      "transformer_blocks.4.attn.norm_k: 0.00 MB\n",
      "transformer_blocks.4.attn.norm_added_q: 0.00 MB\n",
      "transformer_blocks.4.attn.norm_added_k: 0.00 MB\n",
      "transformer_blocks.5.attn.norm_q: 0.00 MB\n",
      "transformer_blocks.5.attn.norm_k: 0.00 MB\n",
      "transformer_blocks.5.attn.norm_added_q: 0.00 MB\n",
      "transformer_blocks.5.attn.norm_added_k: 0.00 MB\n",
      "transformer_blocks.6.attn.norm_q: 0.00 MB\n",
      "transformer_blocks.6.attn.norm_k: 0.00 MB\n",
      "transformer_blocks.6.attn.norm_added_q: 0.00 MB\n",
      "transformer_blocks.6.attn.norm_added_k: 0.00 MB\n",
      "transformer_blocks.7.attn.norm_q: 0.00 MB\n",
      "transformer_blocks.7.attn.norm_k: 0.00 MB\n",
      "transformer_blocks.7.attn.norm_added_q: 0.00 MB\n",
      "transformer_blocks.7.attn.norm_added_k: 0.00 MB\n",
      "transformer_blocks.8.attn.norm_q: 0.00 MB\n",
      "transformer_blocks.8.attn.norm_k: 0.00 MB\n",
      "transformer_blocks.8.attn.norm_added_q: 0.00 MB\n",
      "transformer_blocks.8.attn.norm_added_k: 0.00 MB\n",
      "transformer_blocks.9.attn.norm_q: 0.00 MB\n",
      "transformer_blocks.9.attn.norm_k: 0.00 MB\n",
      "transformer_blocks.9.attn.norm_added_q: 0.00 MB\n",
      "transformer_blocks.9.attn.norm_added_k: 0.00 MB\n",
      "transformer_blocks.10.attn.norm_q: 0.00 MB\n",
      "transformer_blocks.10.attn.norm_k: 0.00 MB\n",
      "transformer_blocks.10.attn.norm_added_q: 0.00 MB\n",
      "transformer_blocks.10.attn.norm_added_k: 0.00 MB\n",
      "transformer_blocks.11.attn.norm_q: 0.00 MB\n",
      "transformer_blocks.11.attn.norm_k: 0.00 MB\n",
      "transformer_blocks.11.attn.norm_added_q: 0.00 MB\n",
      "transformer_blocks.11.attn.norm_added_k: 0.00 MB\n",
      "transformer_blocks.12.attn.norm_q: 0.00 MB\n",
      "transformer_blocks.12.attn.norm_k: 0.00 MB\n",
      "transformer_blocks.12.attn.norm_added_q: 0.00 MB\n",
      "transformer_blocks.12.attn.norm_added_k: 0.00 MB\n",
      "transformer_blocks.13.attn.norm_q: 0.00 MB\n",
      "transformer_blocks.13.attn.norm_k: 0.00 MB\n",
      "transformer_blocks.13.attn.norm_added_q: 0.00 MB\n",
      "transformer_blocks.13.attn.norm_added_k: 0.00 MB\n",
      "transformer_blocks.14.attn.norm_q: 0.00 MB\n",
      "transformer_blocks.14.attn.norm_k: 0.00 MB\n",
      "transformer_blocks.14.attn.norm_added_q: 0.00 MB\n",
      "transformer_blocks.14.attn.norm_added_k: 0.00 MB\n",
      "transformer_blocks.15.attn.norm_q: 0.00 MB\n",
      "transformer_blocks.15.attn.norm_k: 0.00 MB\n",
      "transformer_blocks.15.attn.norm_added_q: 0.00 MB\n",
      "transformer_blocks.15.attn.norm_added_k: 0.00 MB\n",
      "transformer_blocks.16.attn.norm_q: 0.00 MB\n",
      "transformer_blocks.16.attn.norm_k: 0.00 MB\n",
      "transformer_blocks.16.attn.norm_added_q: 0.00 MB\n",
      "transformer_blocks.16.attn.norm_added_k: 0.00 MB\n",
      "transformer_blocks.17.attn.norm_q: 0.00 MB\n",
      "transformer_blocks.17.attn.norm_k: 0.00 MB\n",
      "transformer_blocks.17.attn.norm_added_q: 0.00 MB\n",
      "transformer_blocks.17.attn.norm_added_k: 0.00 MB\n",
      "transformer_blocks.18.attn.norm_q: 0.00 MB\n",
      "transformer_blocks.18.attn.norm_k: 0.00 MB\n",
      "transformer_blocks.18.attn.norm_added_q: 0.00 MB\n",
      "transformer_blocks.18.attn.norm_added_k: 0.00 MB\n",
      "single_transformer_blocks.0.attn.norm_q: 0.00 MB\n",
      "single_transformer_blocks.0.attn.norm_k: 0.00 MB\n",
      "single_transformer_blocks.1.attn.norm_q: 0.00 MB\n",
      "single_transformer_blocks.1.attn.norm_k: 0.00 MB\n",
      "single_transformer_blocks.2.attn.norm_q: 0.00 MB\n",
      "single_transformer_blocks.2.attn.norm_k: 0.00 MB\n",
      "single_transformer_blocks.3.attn.norm_q: 0.00 MB\n",
      "single_transformer_blocks.3.attn.norm_k: 0.00 MB\n",
      "single_transformer_blocks.4.attn.norm_q: 0.00 MB\n",
      "single_transformer_blocks.4.attn.norm_k: 0.00 MB\n",
      "single_transformer_blocks.5.attn.norm_q: 0.00 MB\n",
      "single_transformer_blocks.5.attn.norm_k: 0.00 MB\n",
      "single_transformer_blocks.6.attn.norm_q: 0.00 MB\n",
      "single_transformer_blocks.6.attn.norm_k: 0.00 MB\n",
      "single_transformer_blocks.7.attn.norm_q: 0.00 MB\n",
      "single_transformer_blocks.7.attn.norm_k: 0.00 MB\n",
      "single_transformer_blocks.8.attn.norm_q: 0.00 MB\n",
      "single_transformer_blocks.8.attn.norm_k: 0.00 MB\n",
      "single_transformer_blocks.9.attn.norm_q: 0.00 MB\n",
      "single_transformer_blocks.9.attn.norm_k: 0.00 MB\n",
      "single_transformer_blocks.10.attn.norm_q: 0.00 MB\n",
      "single_transformer_blocks.10.attn.norm_k: 0.00 MB\n",
      "single_transformer_blocks.11.attn.norm_q: 0.00 MB\n",
      "single_transformer_blocks.11.attn.norm_k: 0.00 MB\n",
      "single_transformer_blocks.12.attn.norm_q: 0.00 MB\n",
      "single_transformer_blocks.12.attn.norm_k: 0.00 MB\n",
      "single_transformer_blocks.13.attn.norm_q: 0.00 MB\n",
      "single_transformer_blocks.13.attn.norm_k: 0.00 MB\n",
      "single_transformer_blocks.14.attn.norm_q: 0.00 MB\n",
      "single_transformer_blocks.14.attn.norm_k: 0.00 MB\n",
      "single_transformer_blocks.15.attn.norm_q: 0.00 MB\n",
      "single_transformer_blocks.15.attn.norm_k: 0.00 MB\n",
      "single_transformer_blocks.16.attn.norm_q: 0.00 MB\n",
      "single_transformer_blocks.16.attn.norm_k: 0.00 MB\n",
      "single_transformer_blocks.17.attn.norm_q: 0.00 MB\n",
      "single_transformer_blocks.17.attn.norm_k: 0.00 MB\n",
      "single_transformer_blocks.18.attn.norm_q: 0.00 MB\n",
      "single_transformer_blocks.18.attn.norm_k: 0.00 MB\n",
      "single_transformer_blocks.19.attn.norm_q: 0.00 MB\n",
      "single_transformer_blocks.19.attn.norm_k: 0.00 MB\n",
      "single_transformer_blocks.20.attn.norm_q: 0.00 MB\n",
      "single_transformer_blocks.20.attn.norm_k: 0.00 MB\n",
      "single_transformer_blocks.21.attn.norm_q: 0.00 MB\n",
      "single_transformer_blocks.21.attn.norm_k: 0.00 MB\n",
      "single_transformer_blocks.22.attn.norm_q: 0.00 MB\n",
      "single_transformer_blocks.22.attn.norm_k: 0.00 MB\n",
      "single_transformer_blocks.23.attn.norm_q: 0.00 MB\n",
      "single_transformer_blocks.23.attn.norm_k: 0.00 MB\n",
      "single_transformer_blocks.24.attn.norm_q: 0.00 MB\n",
      "single_transformer_blocks.24.attn.norm_k: 0.00 MB\n",
      "single_transformer_blocks.25.attn.norm_q: 0.00 MB\n",
      "single_transformer_blocks.25.attn.norm_k: 0.00 MB\n",
      "single_transformer_blocks.26.attn.norm_q: 0.00 MB\n",
      "single_transformer_blocks.26.attn.norm_k: 0.00 MB\n",
      "single_transformer_blocks.27.attn.norm_q: 0.00 MB\n",
      "single_transformer_blocks.27.attn.norm_k: 0.00 MB\n",
      "single_transformer_blocks.28.attn.norm_q: 0.00 MB\n",
      "single_transformer_blocks.28.attn.norm_k: 0.00 MB\n",
      "single_transformer_blocks.29.attn.norm_q: 0.00 MB\n",
      "single_transformer_blocks.29.attn.norm_k: 0.00 MB\n",
      "single_transformer_blocks.30.attn.norm_q: 0.00 MB\n",
      "single_transformer_blocks.30.attn.norm_k: 0.00 MB\n",
      "single_transformer_blocks.31.attn.norm_q: 0.00 MB\n",
      "single_transformer_blocks.31.attn.norm_k: 0.00 MB\n",
      "single_transformer_blocks.32.attn.norm_q: 0.00 MB\n",
      "single_transformer_blocks.32.attn.norm_k: 0.00 MB\n",
      "single_transformer_blocks.33.attn.norm_q: 0.00 MB\n",
      "single_transformer_blocks.33.attn.norm_k: 0.00 MB\n",
      "single_transformer_blocks.34.attn.norm_q: 0.00 MB\n",
      "single_transformer_blocks.34.attn.norm_k: 0.00 MB\n",
      "single_transformer_blocks.35.attn.norm_q: 0.00 MB\n",
      "single_transformer_blocks.35.attn.norm_k: 0.00 MB\n",
      "single_transformer_blocks.36.attn.norm_q: 0.00 MB\n",
      "single_transformer_blocks.36.attn.norm_k: 0.00 MB\n",
      "single_transformer_blocks.37.attn.norm_q: 0.00 MB\n",
      "single_transformer_blocks.37.attn.norm_k: 0.00 MB\n",
      "pos_embed: 0.00 MB\n",
      "time_text_embed.time_proj: 0.00 MB\n",
      "time_text_embed.timestep_embedder.act: 0.00 MB\n",
      "time_text_embed.text_embedder.act_1: 0.00 MB\n",
      "transformer_blocks.0.norm1.silu: 0.00 MB\n",
      "transformer_blocks.0.norm1.norm: 0.00 MB\n",
      "transformer_blocks.0.norm1_context.silu: 0.00 MB\n",
      "transformer_blocks.0.norm1_context.norm: 0.00 MB\n",
      "transformer_blocks.0.attn.to_out.1: 0.00 MB\n",
      "transformer_blocks.0.norm2: 0.00 MB\n",
      "transformer_blocks.0.ff.net.1: 0.00 MB\n",
      "transformer_blocks.0.norm2_context: 0.00 MB\n",
      "transformer_blocks.0.ff_context.net.1: 0.00 MB\n",
      "transformer_blocks.1.norm1.silu: 0.00 MB\n",
      "transformer_blocks.1.norm1.norm: 0.00 MB\n",
      "transformer_blocks.1.norm1_context.silu: 0.00 MB\n",
      "transformer_blocks.1.norm1_context.norm: 0.00 MB\n",
      "transformer_blocks.1.attn.to_out.1: 0.00 MB\n",
      "transformer_blocks.1.norm2: 0.00 MB\n",
      "transformer_blocks.1.ff.net.1: 0.00 MB\n",
      "transformer_blocks.1.norm2_context: 0.00 MB\n",
      "transformer_blocks.1.ff_context.net.1: 0.00 MB\n",
      "transformer_blocks.2.norm1.silu: 0.00 MB\n",
      "transformer_blocks.2.norm1.norm: 0.00 MB\n",
      "transformer_blocks.2.norm1_context.silu: 0.00 MB\n",
      "transformer_blocks.2.norm1_context.norm: 0.00 MB\n",
      "transformer_blocks.2.attn.to_out.1: 0.00 MB\n",
      "transformer_blocks.2.norm2: 0.00 MB\n",
      "transformer_blocks.2.ff.net.1: 0.00 MB\n",
      "transformer_blocks.2.norm2_context: 0.00 MB\n",
      "transformer_blocks.2.ff_context.net.1: 0.00 MB\n",
      "transformer_blocks.3.norm1.silu: 0.00 MB\n",
      "transformer_blocks.3.norm1.norm: 0.00 MB\n",
      "transformer_blocks.3.norm1_context.silu: 0.00 MB\n",
      "transformer_blocks.3.norm1_context.norm: 0.00 MB\n",
      "transformer_blocks.3.attn.to_out.1: 0.00 MB\n",
      "transformer_blocks.3.norm2: 0.00 MB\n",
      "transformer_blocks.3.ff.net.1: 0.00 MB\n",
      "transformer_blocks.3.norm2_context: 0.00 MB\n",
      "transformer_blocks.3.ff_context.net.1: 0.00 MB\n",
      "transformer_blocks.4.norm1.silu: 0.00 MB\n",
      "transformer_blocks.4.norm1.norm: 0.00 MB\n",
      "transformer_blocks.4.norm1_context.silu: 0.00 MB\n",
      "transformer_blocks.4.norm1_context.norm: 0.00 MB\n",
      "transformer_blocks.4.attn.to_out.1: 0.00 MB\n",
      "transformer_blocks.4.norm2: 0.00 MB\n",
      "transformer_blocks.4.ff.net.1: 0.00 MB\n",
      "transformer_blocks.4.norm2_context: 0.00 MB\n",
      "transformer_blocks.4.ff_context.net.1: 0.00 MB\n",
      "transformer_blocks.5.norm1.silu: 0.00 MB\n",
      "transformer_blocks.5.norm1.norm: 0.00 MB\n",
      "transformer_blocks.5.norm1_context.silu: 0.00 MB\n",
      "transformer_blocks.5.norm1_context.norm: 0.00 MB\n",
      "transformer_blocks.5.attn.to_out.1: 0.00 MB\n",
      "transformer_blocks.5.norm2: 0.00 MB\n",
      "transformer_blocks.5.ff.net.1: 0.00 MB\n",
      "transformer_blocks.5.norm2_context: 0.00 MB\n",
      "transformer_blocks.5.ff_context.net.1: 0.00 MB\n",
      "transformer_blocks.6.norm1.silu: 0.00 MB\n",
      "transformer_blocks.6.norm1.norm: 0.00 MB\n",
      "transformer_blocks.6.norm1_context.silu: 0.00 MB\n",
      "transformer_blocks.6.norm1_context.norm: 0.00 MB\n",
      "transformer_blocks.6.attn.to_out.1: 0.00 MB\n",
      "transformer_blocks.6.norm2: 0.00 MB\n",
      "transformer_blocks.6.ff.net.1: 0.00 MB\n",
      "transformer_blocks.6.norm2_context: 0.00 MB\n",
      "transformer_blocks.6.ff_context.net.1: 0.00 MB\n",
      "transformer_blocks.7.norm1.silu: 0.00 MB\n",
      "transformer_blocks.7.norm1.norm: 0.00 MB\n",
      "transformer_blocks.7.norm1_context.silu: 0.00 MB\n",
      "transformer_blocks.7.norm1_context.norm: 0.00 MB\n",
      "transformer_blocks.7.attn.to_out.1: 0.00 MB\n",
      "transformer_blocks.7.norm2: 0.00 MB\n",
      "transformer_blocks.7.ff.net.1: 0.00 MB\n",
      "transformer_blocks.7.norm2_context: 0.00 MB\n",
      "transformer_blocks.7.ff_context.net.1: 0.00 MB\n",
      "transformer_blocks.8.norm1.silu: 0.00 MB\n",
      "transformer_blocks.8.norm1.norm: 0.00 MB\n",
      "transformer_blocks.8.norm1_context.silu: 0.00 MB\n",
      "transformer_blocks.8.norm1_context.norm: 0.00 MB\n",
      "transformer_blocks.8.attn.to_out.1: 0.00 MB\n",
      "transformer_blocks.8.norm2: 0.00 MB\n",
      "transformer_blocks.8.ff.net.1: 0.00 MB\n",
      "transformer_blocks.8.norm2_context: 0.00 MB\n",
      "transformer_blocks.8.ff_context.net.1: 0.00 MB\n",
      "transformer_blocks.9.norm1.silu: 0.00 MB\n",
      "transformer_blocks.9.norm1.norm: 0.00 MB\n",
      "transformer_blocks.9.norm1_context.silu: 0.00 MB\n",
      "transformer_blocks.9.norm1_context.norm: 0.00 MB\n",
      "transformer_blocks.9.attn.to_out.1: 0.00 MB\n",
      "transformer_blocks.9.norm2: 0.00 MB\n",
      "transformer_blocks.9.ff.net.1: 0.00 MB\n",
      "transformer_blocks.9.norm2_context: 0.00 MB\n",
      "transformer_blocks.9.ff_context.net.1: 0.00 MB\n",
      "transformer_blocks.10.norm1.silu: 0.00 MB\n",
      "transformer_blocks.10.norm1.norm: 0.00 MB\n",
      "transformer_blocks.10.norm1_context.silu: 0.00 MB\n",
      "transformer_blocks.10.norm1_context.norm: 0.00 MB\n",
      "transformer_blocks.10.attn.to_out.1: 0.00 MB\n",
      "transformer_blocks.10.norm2: 0.00 MB\n",
      "transformer_blocks.10.ff.net.1: 0.00 MB\n",
      "transformer_blocks.10.norm2_context: 0.00 MB\n",
      "transformer_blocks.10.ff_context.net.1: 0.00 MB\n",
      "transformer_blocks.11.norm1.silu: 0.00 MB\n",
      "transformer_blocks.11.norm1.norm: 0.00 MB\n",
      "transformer_blocks.11.norm1_context.silu: 0.00 MB\n",
      "transformer_blocks.11.norm1_context.norm: 0.00 MB\n",
      "transformer_blocks.11.attn.to_out.1: 0.00 MB\n",
      "transformer_blocks.11.norm2: 0.00 MB\n",
      "transformer_blocks.11.ff.net.1: 0.00 MB\n",
      "transformer_blocks.11.norm2_context: 0.00 MB\n",
      "transformer_blocks.11.ff_context.net.1: 0.00 MB\n",
      "transformer_blocks.12.norm1.silu: 0.00 MB\n",
      "transformer_blocks.12.norm1.norm: 0.00 MB\n",
      "transformer_blocks.12.norm1_context.silu: 0.00 MB\n",
      "transformer_blocks.12.norm1_context.norm: 0.00 MB\n",
      "transformer_blocks.12.attn.to_out.1: 0.00 MB\n",
      "transformer_blocks.12.norm2: 0.00 MB\n",
      "transformer_blocks.12.ff.net.1: 0.00 MB\n",
      "transformer_blocks.12.norm2_context: 0.00 MB\n",
      "transformer_blocks.12.ff_context.net.1: 0.00 MB\n",
      "transformer_blocks.13.norm1.silu: 0.00 MB\n",
      "transformer_blocks.13.norm1.norm: 0.00 MB\n",
      "transformer_blocks.13.norm1_context.silu: 0.00 MB\n",
      "transformer_blocks.13.norm1_context.norm: 0.00 MB\n",
      "transformer_blocks.13.attn.to_out.1: 0.00 MB\n",
      "transformer_blocks.13.norm2: 0.00 MB\n",
      "transformer_blocks.13.ff.net.1: 0.00 MB\n",
      "transformer_blocks.13.norm2_context: 0.00 MB\n",
      "transformer_blocks.13.ff_context.net.1: 0.00 MB\n",
      "transformer_blocks.14.norm1.silu: 0.00 MB\n",
      "transformer_blocks.14.norm1.norm: 0.00 MB\n",
      "transformer_blocks.14.norm1_context.silu: 0.00 MB\n",
      "transformer_blocks.14.norm1_context.norm: 0.00 MB\n",
      "transformer_blocks.14.attn.to_out.1: 0.00 MB\n",
      "transformer_blocks.14.norm2: 0.00 MB\n",
      "transformer_blocks.14.ff.net.1: 0.00 MB\n",
      "transformer_blocks.14.norm2_context: 0.00 MB\n",
      "transformer_blocks.14.ff_context.net.1: 0.00 MB\n",
      "transformer_blocks.15.norm1.silu: 0.00 MB\n",
      "transformer_blocks.15.norm1.norm: 0.00 MB\n",
      "transformer_blocks.15.norm1_context.silu: 0.00 MB\n",
      "transformer_blocks.15.norm1_context.norm: 0.00 MB\n",
      "transformer_blocks.15.attn.to_out.1: 0.00 MB\n",
      "transformer_blocks.15.norm2: 0.00 MB\n",
      "transformer_blocks.15.ff.net.1: 0.00 MB\n",
      "transformer_blocks.15.norm2_context: 0.00 MB\n",
      "transformer_blocks.15.ff_context.net.1: 0.00 MB\n",
      "transformer_blocks.16.norm1.silu: 0.00 MB\n",
      "transformer_blocks.16.norm1.norm: 0.00 MB\n",
      "transformer_blocks.16.norm1_context.silu: 0.00 MB\n",
      "transformer_blocks.16.norm1_context.norm: 0.00 MB\n",
      "transformer_blocks.16.attn.to_out.1: 0.00 MB\n",
      "transformer_blocks.16.norm2: 0.00 MB\n",
      "transformer_blocks.16.ff.net.1: 0.00 MB\n",
      "transformer_blocks.16.norm2_context: 0.00 MB\n",
      "transformer_blocks.16.ff_context.net.1: 0.00 MB\n",
      "transformer_blocks.17.norm1.silu: 0.00 MB\n",
      "transformer_blocks.17.norm1.norm: 0.00 MB\n",
      "transformer_blocks.17.norm1_context.silu: 0.00 MB\n",
      "transformer_blocks.17.norm1_context.norm: 0.00 MB\n",
      "transformer_blocks.17.attn.to_out.1: 0.00 MB\n",
      "transformer_blocks.17.norm2: 0.00 MB\n",
      "transformer_blocks.17.ff.net.1: 0.00 MB\n",
      "transformer_blocks.17.norm2_context: 0.00 MB\n",
      "transformer_blocks.17.ff_context.net.1: 0.00 MB\n",
      "transformer_blocks.18.norm1.silu: 0.00 MB\n",
      "transformer_blocks.18.norm1.norm: 0.00 MB\n",
      "transformer_blocks.18.norm1_context.silu: 0.00 MB\n",
      "transformer_blocks.18.norm1_context.norm: 0.00 MB\n",
      "transformer_blocks.18.attn.to_out.1: 0.00 MB\n",
      "transformer_blocks.18.norm2: 0.00 MB\n",
      "transformer_blocks.18.ff.net.1: 0.00 MB\n",
      "transformer_blocks.18.norm2_context: 0.00 MB\n",
      "transformer_blocks.18.ff_context.net.1: 0.00 MB\n",
      "single_transformer_blocks.0.norm.silu: 0.00 MB\n",
      "single_transformer_blocks.0.norm.norm: 0.00 MB\n",
      "single_transformer_blocks.0.act_mlp: 0.00 MB\n",
      "single_transformer_blocks.1.norm.silu: 0.00 MB\n",
      "single_transformer_blocks.1.norm.norm: 0.00 MB\n",
      "single_transformer_blocks.1.act_mlp: 0.00 MB\n",
      "single_transformer_blocks.2.norm.silu: 0.00 MB\n",
      "single_transformer_blocks.2.norm.norm: 0.00 MB\n",
      "single_transformer_blocks.2.act_mlp: 0.00 MB\n",
      "single_transformer_blocks.3.norm.silu: 0.00 MB\n",
      "single_transformer_blocks.3.norm.norm: 0.00 MB\n",
      "single_transformer_blocks.3.act_mlp: 0.00 MB\n",
      "single_transformer_blocks.4.norm.silu: 0.00 MB\n",
      "single_transformer_blocks.4.norm.norm: 0.00 MB\n",
      "single_transformer_blocks.4.act_mlp: 0.00 MB\n",
      "single_transformer_blocks.5.norm.silu: 0.00 MB\n",
      "single_transformer_blocks.5.norm.norm: 0.00 MB\n",
      "single_transformer_blocks.5.act_mlp: 0.00 MB\n",
      "single_transformer_blocks.6.norm.silu: 0.00 MB\n",
      "single_transformer_blocks.6.norm.norm: 0.00 MB\n",
      "single_transformer_blocks.6.act_mlp: 0.00 MB\n",
      "single_transformer_blocks.7.norm.silu: 0.00 MB\n",
      "single_transformer_blocks.7.norm.norm: 0.00 MB\n",
      "single_transformer_blocks.7.act_mlp: 0.00 MB\n",
      "single_transformer_blocks.8.norm.silu: 0.00 MB\n",
      "single_transformer_blocks.8.norm.norm: 0.00 MB\n",
      "single_transformer_blocks.8.act_mlp: 0.00 MB\n",
      "single_transformer_blocks.9.norm.silu: 0.00 MB\n",
      "single_transformer_blocks.9.norm.norm: 0.00 MB\n",
      "single_transformer_blocks.9.act_mlp: 0.00 MB\n",
      "single_transformer_blocks.10.norm.silu: 0.00 MB\n",
      "single_transformer_blocks.10.norm.norm: 0.00 MB\n",
      "single_transformer_blocks.10.act_mlp: 0.00 MB\n",
      "single_transformer_blocks.11.norm.silu: 0.00 MB\n",
      "single_transformer_blocks.11.norm.norm: 0.00 MB\n",
      "single_transformer_blocks.11.act_mlp: 0.00 MB\n",
      "single_transformer_blocks.12.norm.silu: 0.00 MB\n",
      "single_transformer_blocks.12.norm.norm: 0.00 MB\n",
      "single_transformer_blocks.12.act_mlp: 0.00 MB\n",
      "single_transformer_blocks.13.norm.silu: 0.00 MB\n",
      "single_transformer_blocks.13.norm.norm: 0.00 MB\n",
      "single_transformer_blocks.13.act_mlp: 0.00 MB\n",
      "single_transformer_blocks.14.norm.silu: 0.00 MB\n",
      "single_transformer_blocks.14.norm.norm: 0.00 MB\n",
      "single_transformer_blocks.14.act_mlp: 0.00 MB\n",
      "single_transformer_blocks.15.norm.silu: 0.00 MB\n",
      "single_transformer_blocks.15.norm.norm: 0.00 MB\n",
      "single_transformer_blocks.15.act_mlp: 0.00 MB\n",
      "single_transformer_blocks.16.norm.silu: 0.00 MB\n",
      "single_transformer_blocks.16.norm.norm: 0.00 MB\n",
      "single_transformer_blocks.16.act_mlp: 0.00 MB\n",
      "single_transformer_blocks.17.norm.silu: 0.00 MB\n",
      "single_transformer_blocks.17.norm.norm: 0.00 MB\n",
      "single_transformer_blocks.17.act_mlp: 0.00 MB\n",
      "single_transformer_blocks.18.norm.silu: 0.00 MB\n",
      "single_transformer_blocks.18.norm.norm: 0.00 MB\n",
      "single_transformer_blocks.18.act_mlp: 0.00 MB\n",
      "single_transformer_blocks.19.norm.silu: 0.00 MB\n",
      "single_transformer_blocks.19.norm.norm: 0.00 MB\n",
      "single_transformer_blocks.19.act_mlp: 0.00 MB\n",
      "single_transformer_blocks.20.norm.silu: 0.00 MB\n",
      "single_transformer_blocks.20.norm.norm: 0.00 MB\n",
      "single_transformer_blocks.20.act_mlp: 0.00 MB\n",
      "single_transformer_blocks.21.norm.silu: 0.00 MB\n",
      "single_transformer_blocks.21.norm.norm: 0.00 MB\n",
      "single_transformer_blocks.21.act_mlp: 0.00 MB\n",
      "single_transformer_blocks.22.norm.silu: 0.00 MB\n",
      "single_transformer_blocks.22.norm.norm: 0.00 MB\n",
      "single_transformer_blocks.22.act_mlp: 0.00 MB\n",
      "single_transformer_blocks.23.norm.silu: 0.00 MB\n",
      "single_transformer_blocks.23.norm.norm: 0.00 MB\n",
      "single_transformer_blocks.23.act_mlp: 0.00 MB\n",
      "single_transformer_blocks.24.norm.silu: 0.00 MB\n",
      "single_transformer_blocks.24.norm.norm: 0.00 MB\n",
      "single_transformer_blocks.24.act_mlp: 0.00 MB\n",
      "single_transformer_blocks.25.norm.silu: 0.00 MB\n",
      "single_transformer_blocks.25.norm.norm: 0.00 MB\n",
      "single_transformer_blocks.25.act_mlp: 0.00 MB\n",
      "single_transformer_blocks.26.norm.silu: 0.00 MB\n",
      "single_transformer_blocks.26.norm.norm: 0.00 MB\n",
      "single_transformer_blocks.26.act_mlp: 0.00 MB\n",
      "single_transformer_blocks.27.norm.silu: 0.00 MB\n",
      "single_transformer_blocks.27.norm.norm: 0.00 MB\n",
      "single_transformer_blocks.27.act_mlp: 0.00 MB\n",
      "single_transformer_blocks.28.norm.silu: 0.00 MB\n",
      "single_transformer_blocks.28.norm.norm: 0.00 MB\n",
      "single_transformer_blocks.28.act_mlp: 0.00 MB\n",
      "single_transformer_blocks.29.norm.silu: 0.00 MB\n",
      "single_transformer_blocks.29.norm.norm: 0.00 MB\n",
      "single_transformer_blocks.29.act_mlp: 0.00 MB\n",
      "single_transformer_blocks.30.norm.silu: 0.00 MB\n",
      "single_transformer_blocks.30.norm.norm: 0.00 MB\n",
      "single_transformer_blocks.30.act_mlp: 0.00 MB\n",
      "single_transformer_blocks.31.norm.silu: 0.00 MB\n",
      "single_transformer_blocks.31.norm.norm: 0.00 MB\n",
      "single_transformer_blocks.31.act_mlp: 0.00 MB\n",
      "single_transformer_blocks.32.norm.silu: 0.00 MB\n",
      "single_transformer_blocks.32.norm.norm: 0.00 MB\n",
      "single_transformer_blocks.32.act_mlp: 0.00 MB\n",
      "single_transformer_blocks.33.norm.silu: 0.00 MB\n",
      "single_transformer_blocks.33.norm.norm: 0.00 MB\n",
      "single_transformer_blocks.33.act_mlp: 0.00 MB\n",
      "single_transformer_blocks.34.norm.silu: 0.00 MB\n",
      "single_transformer_blocks.34.norm.norm: 0.00 MB\n",
      "single_transformer_blocks.34.act_mlp: 0.00 MB\n",
      "single_transformer_blocks.35.norm.silu: 0.00 MB\n",
      "single_transformer_blocks.35.norm.norm: 0.00 MB\n",
      "single_transformer_blocks.35.act_mlp: 0.00 MB\n",
      "single_transformer_blocks.36.norm.silu: 0.00 MB\n",
      "single_transformer_blocks.36.norm.norm: 0.00 MB\n",
      "single_transformer_blocks.36.act_mlp: 0.00 MB\n",
      "single_transformer_blocks.37.norm.silu: 0.00 MB\n",
      "single_transformer_blocks.37.norm.norm: 0.00 MB\n",
      "single_transformer_blocks.37.act_mlp: 0.00 MB\n",
      "norm_out.silu: 0.00 MB\n",
      "norm_out.norm: 0.00 MB\n",
      "\n",
      "Total model size before quantization: 92929.16 MB\n",
      "\n",
      "Quantizing transformer_blocks (12315.36 MB)...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.ao.quantization import quantize_dynamic\n",
    "import gc\n",
    "import numpy as np\n",
    "\n",
    "def get_model_size(model):\n",
    "    param_size = 0\n",
    "    for param in model.parameters():\n",
    "        param_size += param.nelement() * param.element_size()\n",
    "    buffer_size = 0\n",
    "    for buffer in model.buffers():\n",
    "        buffer_size += buffer.nelement() * buffer.element_size()\n",
    "    size_all_mb = (param_size + buffer_size) / 1024**2\n",
    "    return size_all_mb\n",
    "\n",
    "def check_submodule_sizes(model, parent_name=''):\n",
    "    sizes = []\n",
    "    for name, submodule in model.named_children():\n",
    "        full_name = f\"{parent_name}.{name}\" if parent_name else name\n",
    "        size_mb = get_model_size(submodule)\n",
    "        sizes.append((full_name, size_mb))\n",
    "        sizes.extend(check_submodule_sizes(submodule, full_name))\n",
    "    return sizes\n",
    "\n",
    "def quantize_module(module, dtype=torch.qint8):\n",
    "    return quantize_dynamic(\n",
    "        module,\n",
    "        {torch.nn.Linear},\n",
    "        dtype=dtype\n",
    "    )\n",
    "\n",
    "def quantize_transformer_submodules(transformer, dtype=torch.qint8, size_threshold_mb=100):\n",
    "    sizes = check_submodule_sizes(transformer)\n",
    "    sizes.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # print(\"Submodule sizes before quantization:\")\n",
    "    # for name, size in sizes:\n",
    "    #     print(f\"{name}: {size:.2f} MB\")\n",
    "    \n",
    "    total_size_before = sum(size for _, size in sizes)\n",
    "    print(f\"\\nTotal model size before quantization: {total_size_before:.2f} MB\")\n",
    "    \n",
    "    for name, size in sizes:\n",
    "        if size > size_threshold_mb:\n",
    "            print(f\"\\nQuantizing {name} ({size:.2f} MB)...\")\n",
    "            submodule = transformer\n",
    "            for part in name.split('.'):\n",
    "                submodule = getattr(submodule, part)\n",
    "            setattr(submodule, part, quantize_module(submodule, dtype))\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    sizes_after = check_submodule_sizes(transformer)\n",
    "    total_size_after = sum(size for _, size in sizes_after)\n",
    "    print(f\"\\nTotal model size after quantization: {total_size_after:.2f} MB\")\n",
    "    print(f\"Size reduction: {(1 - total_size_after/total_size_before)*100:.2f}%\")\n",
    "    \n",
    "    return transformer\n",
    "\n",
    "# Assuming 'pipe' is your pipeline object\n",
    "# Move to CPU for quantization\n",
    "pipe.to('cpu')\n",
    "\n",
    "# Clear GPU memory\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "try:\n",
    "    # Check sizes and quantize transformer submodules\n",
    "    pipe.transformer = quantize_transformer_submodules(pipe.transformer, size_threshold_mb=100)\n",
    "    print(\"Quantization successful\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during quantization: {e}\")\n",
    "\n",
    "# Move back to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "pipe.to(device)\n",
    "\n",
    "# Example usage\n",
    "prompt = \"A serene landscape with mountains and a lake\"\n",
    "image = pipe(prompt).images[0]\n",
    "image.save(\"generated_image.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a03df56-0a6d-43b6-9cb9-ec3b1de54084",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "71a194d7-e6a7-4f80-8e7e-246bcb337ceb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-08T09:57:01.718281Z",
     "iopub.status.busy": "2024-09-08T09:57:01.717765Z",
     "iopub.status.idle": "2024-09-08T09:57:01.721862Z",
     "shell.execute_reply": "2024-09-08T09:57:01.720858Z",
     "shell.execute_reply.started": "2024-09-08T09:57:01.718281Z"
    }
   },
   "source": [
    "## b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdaf14c9-9bd4-491e-b439-3dbe974a4274",
   "metadata": {
    "execution": {
     "execution_failed": "2024-09-08T10:08:22.420Z",
     "iopub.execute_input": "2024-09-08T10:06:01.233391Z",
     "iopub.status.busy": "2024-09-08T10:06:01.232378Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97460beec7a0487ca2b7fdbc825bfe1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bdf0457a8da4fb3846775c16b7b4bbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total model size before quantization: 92929.16 MB\n",
      "Skipping pos_embed (0.00 MB) - below threshold\n",
      "Quantizing time_text_embed (42.02 MB)...\n",
      "Skipping time_text_embed.time_proj (0.00 MB) - below threshold\n",
      "Quantizing time_text_embed.timestep_embedder (19.51 MB)...\n",
      "Skipping time_text_embed.timestep_embedder.linear_1 (1.51 MB) - below threshold\n",
      "Skipping time_text_embed.timestep_embedder.act (0.00 MB) - below threshold\n",
      "Quantizing time_text_embed.timestep_embedder.linear_2 (18.01 MB)...\n",
      "Quantizing time_text_embed.text_embedder (22.51 MB)...\n",
      "Skipping time_text_embed.text_embedder.linear_1 (4.51 MB) - below threshold\n",
      "Skipping time_text_embed.text_embedder.act_1 (0.00 MB) - below threshold\n",
      "Quantizing time_text_embed.text_embedder.linear_2 (18.01 MB)...\n",
      "Quantizing context_embedder (24.01 MB)...\n",
      "Skipping x_embedder (0.38 MB) - below threshold\n",
      "Quantizing transformer_blocks (12315.36 MB)...\n",
      "Quantizing transformer_blocks.0 (648.18 MB)...\n",
      "Quantizing transformer_blocks.0.norm1 (108.04 MB)...\n",
      "Skipping transformer_blocks.0.norm1.silu (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.0.norm1.linear (108.04 MB)...\n",
      "Skipping transformer_blocks.0.norm1.norm (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.0.norm1_context (108.04 MB)...\n",
      "Skipping transformer_blocks.0.norm1_context.silu (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.0.norm1_context.linear (108.04 MB)...\n",
      "Skipping transformer_blocks.0.norm1_context.norm (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.0.attn (144.05 MB)...\n",
      "Skipping transformer_blocks.0.attn.norm_q (0.00 MB) - below threshold\n",
      "Skipping transformer_blocks.0.attn.norm_k (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.0.attn.to_q (18.01 MB)...\n",
      "Quantizing transformer_blocks.0.attn.to_k (18.01 MB)...\n",
      "Quantizing transformer_blocks.0.attn.to_v (18.01 MB)...\n",
      "Quantizing transformer_blocks.0.attn.add_k_proj (18.01 MB)...\n",
      "Quantizing transformer_blocks.0.attn.add_v_proj (18.01 MB)...\n",
      "Quantizing transformer_blocks.0.attn.add_q_proj (18.01 MB)...\n",
      "Quantizing transformer_blocks.0.attn.to_out (18.01 MB)...\n",
      "Quantizing transformer_blocks.0.attn.to_out.0 (18.01 MB)...\n",
      "Skipping transformer_blocks.0.attn.to_out.1 (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.0.attn.to_add_out (18.01 MB)...\n",
      "Skipping transformer_blocks.0.attn.norm_added_q (0.00 MB) - below threshold\n",
      "Skipping transformer_blocks.0.attn.norm_added_k (0.00 MB) - below threshold\n",
      "Skipping transformer_blocks.0.norm2 (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.0.ff (144.03 MB)...\n",
      "Quantizing transformer_blocks.0.ff.net (144.03 MB)...\n",
      "Quantizing transformer_blocks.0.ff.net.0 (72.02 MB)...\n",
      "Quantizing transformer_blocks.0.ff.net.0.proj (72.02 MB)...\n",
      "Skipping transformer_blocks.0.ff.net.1 (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.0.ff.net.2 (72.01 MB)...\n",
      "Skipping transformer_blocks.0.norm2_context (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.0.ff_context (144.03 MB)...\n",
      "Quantizing transformer_blocks.0.ff_context.net (144.03 MB)...\n",
      "Quantizing transformer_blocks.0.ff_context.net.0 (72.02 MB)...\n",
      "Quantizing transformer_blocks.0.ff_context.net.0.proj (72.02 MB)...\n",
      "Skipping transformer_blocks.0.ff_context.net.1 (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.0.ff_context.net.2 (72.01 MB)...\n",
      "Quantizing transformer_blocks.1 (648.18 MB)...\n",
      "Quantizing transformer_blocks.1.norm1 (108.04 MB)...\n",
      "Skipping transformer_blocks.1.norm1.silu (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.1.norm1.linear (108.04 MB)...\n",
      "Skipping transformer_blocks.1.norm1.norm (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.1.norm1_context (108.04 MB)...\n",
      "Skipping transformer_blocks.1.norm1_context.silu (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.1.norm1_context.linear (108.04 MB)...\n",
      "Skipping transformer_blocks.1.norm1_context.norm (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.1.attn (144.05 MB)...\n",
      "Skipping transformer_blocks.1.attn.norm_q (0.00 MB) - below threshold\n",
      "Skipping transformer_blocks.1.attn.norm_k (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.1.attn.to_q (18.01 MB)...\n",
      "Quantizing transformer_blocks.1.attn.to_k (18.01 MB)...\n",
      "Quantizing transformer_blocks.1.attn.to_v (18.01 MB)...\n",
      "Quantizing transformer_blocks.1.attn.add_k_proj (18.01 MB)...\n",
      "Quantizing transformer_blocks.1.attn.add_v_proj (18.01 MB)...\n",
      "Quantizing transformer_blocks.1.attn.add_q_proj (18.01 MB)...\n",
      "Quantizing transformer_blocks.1.attn.to_out (18.01 MB)...\n",
      "Quantizing transformer_blocks.1.attn.to_out.0 (18.01 MB)...\n",
      "Skipping transformer_blocks.1.attn.to_out.1 (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.1.attn.to_add_out (18.01 MB)...\n",
      "Skipping transformer_blocks.1.attn.norm_added_q (0.00 MB) - below threshold\n",
      "Skipping transformer_blocks.1.attn.norm_added_k (0.00 MB) - below threshold\n",
      "Skipping transformer_blocks.1.norm2 (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.1.ff (144.03 MB)...\n",
      "Quantizing transformer_blocks.1.ff.net (144.03 MB)...\n",
      "Quantizing transformer_blocks.1.ff.net.0 (72.02 MB)...\n",
      "Quantizing transformer_blocks.1.ff.net.0.proj (72.02 MB)...\n",
      "Skipping transformer_blocks.1.ff.net.1 (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.1.ff.net.2 (72.01 MB)...\n",
      "Skipping transformer_blocks.1.norm2_context (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.1.ff_context (144.03 MB)...\n",
      "Quantizing transformer_blocks.1.ff_context.net (144.03 MB)...\n",
      "Quantizing transformer_blocks.1.ff_context.net.0 (72.02 MB)...\n",
      "Quantizing transformer_blocks.1.ff_context.net.0.proj (72.02 MB)...\n",
      "Skipping transformer_blocks.1.ff_context.net.1 (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.1.ff_context.net.2 (72.01 MB)...\n",
      "Quantizing transformer_blocks.2 (648.18 MB)...\n",
      "Quantizing transformer_blocks.2.norm1 (108.04 MB)...\n",
      "Skipping transformer_blocks.2.norm1.silu (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.2.norm1.linear (108.04 MB)...\n",
      "Skipping transformer_blocks.2.norm1.norm (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.2.norm1_context (108.04 MB)...\n",
      "Skipping transformer_blocks.2.norm1_context.silu (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.2.norm1_context.linear (108.04 MB)...\n",
      "Skipping transformer_blocks.2.norm1_context.norm (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.2.attn (144.05 MB)...\n",
      "Skipping transformer_blocks.2.attn.norm_q (0.00 MB) - below threshold\n",
      "Skipping transformer_blocks.2.attn.norm_k (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.2.attn.to_q (18.01 MB)...\n",
      "Quantizing transformer_blocks.2.attn.to_k (18.01 MB)...\n",
      "Quantizing transformer_blocks.2.attn.to_v (18.01 MB)...\n",
      "Quantizing transformer_blocks.2.attn.add_k_proj (18.01 MB)...\n",
      "Quantizing transformer_blocks.2.attn.add_v_proj (18.01 MB)...\n",
      "Quantizing transformer_blocks.2.attn.add_q_proj (18.01 MB)...\n",
      "Quantizing transformer_blocks.2.attn.to_out (18.01 MB)...\n",
      "Quantizing transformer_blocks.2.attn.to_out.0 (18.01 MB)...\n",
      "Skipping transformer_blocks.2.attn.to_out.1 (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.2.attn.to_add_out (18.01 MB)...\n",
      "Skipping transformer_blocks.2.attn.norm_added_q (0.00 MB) - below threshold\n",
      "Skipping transformer_blocks.2.attn.norm_added_k (0.00 MB) - below threshold\n",
      "Skipping transformer_blocks.2.norm2 (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.2.ff (144.03 MB)...\n",
      "Quantizing transformer_blocks.2.ff.net (144.03 MB)...\n",
      "Quantizing transformer_blocks.2.ff.net.0 (72.02 MB)...\n",
      "Quantizing transformer_blocks.2.ff.net.0.proj (72.02 MB)...\n",
      "Skipping transformer_blocks.2.ff.net.1 (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.2.ff.net.2 (72.01 MB)...\n",
      "Skipping transformer_blocks.2.norm2_context (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.2.ff_context (144.03 MB)...\n",
      "Quantizing transformer_blocks.2.ff_context.net (144.03 MB)...\n",
      "Quantizing transformer_blocks.2.ff_context.net.0 (72.02 MB)...\n",
      "Quantizing transformer_blocks.2.ff_context.net.0.proj (72.02 MB)...\n",
      "Skipping transformer_blocks.2.ff_context.net.1 (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.2.ff_context.net.2 (72.01 MB)...\n",
      "Quantizing transformer_blocks.3 (648.18 MB)...\n",
      "Quantizing transformer_blocks.3.norm1 (108.04 MB)...\n",
      "Skipping transformer_blocks.3.norm1.silu (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.3.norm1.linear (108.04 MB)...\n",
      "Skipping transformer_blocks.3.norm1.norm (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.3.norm1_context (108.04 MB)...\n",
      "Skipping transformer_blocks.3.norm1_context.silu (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.3.norm1_context.linear (108.04 MB)...\n",
      "Skipping transformer_blocks.3.norm1_context.norm (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.3.attn (144.05 MB)...\n",
      "Skipping transformer_blocks.3.attn.norm_q (0.00 MB) - below threshold\n",
      "Skipping transformer_blocks.3.attn.norm_k (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.3.attn.to_q (18.01 MB)...\n",
      "Quantizing transformer_blocks.3.attn.to_k (18.01 MB)...\n",
      "Quantizing transformer_blocks.3.attn.to_v (18.01 MB)...\n",
      "Quantizing transformer_blocks.3.attn.add_k_proj (18.01 MB)...\n",
      "Quantizing transformer_blocks.3.attn.add_v_proj (18.01 MB)...\n",
      "Quantizing transformer_blocks.3.attn.add_q_proj (18.01 MB)...\n",
      "Quantizing transformer_blocks.3.attn.to_out (18.01 MB)...\n",
      "Quantizing transformer_blocks.3.attn.to_out.0 (18.01 MB)...\n",
      "Skipping transformer_blocks.3.attn.to_out.1 (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.3.attn.to_add_out (18.01 MB)...\n",
      "Skipping transformer_blocks.3.attn.norm_added_q (0.00 MB) - below threshold\n",
      "Skipping transformer_blocks.3.attn.norm_added_k (0.00 MB) - below threshold\n",
      "Skipping transformer_blocks.3.norm2 (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.3.ff (144.03 MB)...\n",
      "Quantizing transformer_blocks.3.ff.net (144.03 MB)...\n",
      "Quantizing transformer_blocks.3.ff.net.0 (72.02 MB)...\n",
      "Quantizing transformer_blocks.3.ff.net.0.proj (72.02 MB)...\n",
      "Skipping transformer_blocks.3.ff.net.1 (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.3.ff.net.2 (72.01 MB)...\n",
      "Skipping transformer_blocks.3.norm2_context (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.3.ff_context (144.03 MB)...\n",
      "Quantizing transformer_blocks.3.ff_context.net (144.03 MB)...\n",
      "Quantizing transformer_blocks.3.ff_context.net.0 (72.02 MB)...\n",
      "Quantizing transformer_blocks.3.ff_context.net.0.proj (72.02 MB)...\n",
      "Skipping transformer_blocks.3.ff_context.net.1 (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.3.ff_context.net.2 (72.01 MB)...\n",
      "Quantizing transformer_blocks.4 (648.18 MB)...\n",
      "Quantizing transformer_blocks.4.norm1 (108.04 MB)...\n",
      "Skipping transformer_blocks.4.norm1.silu (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.4.norm1.linear (108.04 MB)...\n",
      "Skipping transformer_blocks.4.norm1.norm (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.4.norm1_context (108.04 MB)...\n",
      "Skipping transformer_blocks.4.norm1_context.silu (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.4.norm1_context.linear (108.04 MB)...\n",
      "Skipping transformer_blocks.4.norm1_context.norm (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.4.attn (144.05 MB)...\n",
      "Skipping transformer_blocks.4.attn.norm_q (0.00 MB) - below threshold\n",
      "Skipping transformer_blocks.4.attn.norm_k (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.4.attn.to_q (18.01 MB)...\n",
      "Quantizing transformer_blocks.4.attn.to_k (18.01 MB)...\n",
      "Quantizing transformer_blocks.4.attn.to_v (18.01 MB)...\n",
      "Quantizing transformer_blocks.4.attn.add_k_proj (18.01 MB)...\n",
      "Quantizing transformer_blocks.4.attn.add_v_proj (18.01 MB)...\n",
      "Quantizing transformer_blocks.4.attn.add_q_proj (18.01 MB)...\n",
      "Quantizing transformer_blocks.4.attn.to_out (18.01 MB)...\n",
      "Quantizing transformer_blocks.4.attn.to_out.0 (18.01 MB)...\n",
      "Skipping transformer_blocks.4.attn.to_out.1 (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.4.attn.to_add_out (18.01 MB)...\n",
      "Skipping transformer_blocks.4.attn.norm_added_q (0.00 MB) - below threshold\n",
      "Skipping transformer_blocks.4.attn.norm_added_k (0.00 MB) - below threshold\n",
      "Skipping transformer_blocks.4.norm2 (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.4.ff (144.03 MB)...\n",
      "Quantizing transformer_blocks.4.ff.net (144.03 MB)...\n",
      "Quantizing transformer_blocks.4.ff.net.0 (72.02 MB)...\n",
      "Quantizing transformer_blocks.4.ff.net.0.proj (72.02 MB)...\n",
      "Skipping transformer_blocks.4.ff.net.1 (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.4.ff.net.2 (72.01 MB)...\n",
      "Skipping transformer_blocks.4.norm2_context (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.4.ff_context (144.03 MB)...\n",
      "Quantizing transformer_blocks.4.ff_context.net (144.03 MB)...\n",
      "Quantizing transformer_blocks.4.ff_context.net.0 (72.02 MB)...\n",
      "Quantizing transformer_blocks.4.ff_context.net.0.proj (72.02 MB)...\n",
      "Skipping transformer_blocks.4.ff_context.net.1 (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.4.ff_context.net.2 (72.01 MB)...\n",
      "Quantizing transformer_blocks.5 (648.18 MB)...\n",
      "Quantizing transformer_blocks.5.norm1 (108.04 MB)...\n",
      "Skipping transformer_blocks.5.norm1.silu (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.5.norm1.linear (108.04 MB)...\n",
      "Skipping transformer_blocks.5.norm1.norm (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.5.norm1_context (108.04 MB)...\n",
      "Skipping transformer_blocks.5.norm1_context.silu (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.5.norm1_context.linear (108.04 MB)...\n",
      "Skipping transformer_blocks.5.norm1_context.norm (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.5.attn (144.05 MB)...\n",
      "Skipping transformer_blocks.5.attn.norm_q (0.00 MB) - below threshold\n",
      "Skipping transformer_blocks.5.attn.norm_k (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.5.attn.to_q (18.01 MB)...\n",
      "Quantizing transformer_blocks.5.attn.to_k (18.01 MB)...\n",
      "Quantizing transformer_blocks.5.attn.to_v (18.01 MB)...\n",
      "Quantizing transformer_blocks.5.attn.add_k_proj (18.01 MB)...\n",
      "Quantizing transformer_blocks.5.attn.add_v_proj (18.01 MB)...\n",
      "Quantizing transformer_blocks.5.attn.add_q_proj (18.01 MB)...\n",
      "Quantizing transformer_blocks.5.attn.to_out (18.01 MB)...\n",
      "Quantizing transformer_blocks.5.attn.to_out.0 (18.01 MB)...\n",
      "Skipping transformer_blocks.5.attn.to_out.1 (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.5.attn.to_add_out (18.01 MB)...\n",
      "Skipping transformer_blocks.5.attn.norm_added_q (0.00 MB) - below threshold\n",
      "Skipping transformer_blocks.5.attn.norm_added_k (0.00 MB) - below threshold\n",
      "Skipping transformer_blocks.5.norm2 (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.5.ff (144.03 MB)...\n",
      "Quantizing transformer_blocks.5.ff.net (144.03 MB)...\n",
      "Quantizing transformer_blocks.5.ff.net.0 (72.02 MB)...\n",
      "Quantizing transformer_blocks.5.ff.net.0.proj (72.02 MB)...\n",
      "Skipping transformer_blocks.5.ff.net.1 (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.5.ff.net.2 (72.01 MB)...\n",
      "Skipping transformer_blocks.5.norm2_context (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.5.ff_context (144.03 MB)...\n",
      "Quantizing transformer_blocks.5.ff_context.net (144.03 MB)...\n",
      "Quantizing transformer_blocks.5.ff_context.net.0 (72.02 MB)...\n",
      "Quantizing transformer_blocks.5.ff_context.net.0.proj (72.02 MB)...\n",
      "Skipping transformer_blocks.5.ff_context.net.1 (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.5.ff_context.net.2 (72.01 MB)...\n",
      "Quantizing transformer_blocks.6 (648.18 MB)...\n",
      "Quantizing transformer_blocks.6.norm1 (108.04 MB)...\n",
      "Skipping transformer_blocks.6.norm1.silu (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.6.norm1.linear (108.04 MB)...\n",
      "Skipping transformer_blocks.6.norm1.norm (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.6.norm1_context (108.04 MB)...\n",
      "Skipping transformer_blocks.6.norm1_context.silu (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.6.norm1_context.linear (108.04 MB)...\n",
      "Skipping transformer_blocks.6.norm1_context.norm (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.6.attn (144.05 MB)...\n",
      "Skipping transformer_blocks.6.attn.norm_q (0.00 MB) - below threshold\n",
      "Skipping transformer_blocks.6.attn.norm_k (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.6.attn.to_q (18.01 MB)...\n",
      "Quantizing transformer_blocks.6.attn.to_k (18.01 MB)...\n",
      "Quantizing transformer_blocks.6.attn.to_v (18.01 MB)...\n",
      "Quantizing transformer_blocks.6.attn.add_k_proj (18.01 MB)...\n",
      "Quantizing transformer_blocks.6.attn.add_v_proj (18.01 MB)...\n",
      "Quantizing transformer_blocks.6.attn.add_q_proj (18.01 MB)...\n",
      "Quantizing transformer_blocks.6.attn.to_out (18.01 MB)...\n",
      "Quantizing transformer_blocks.6.attn.to_out.0 (18.01 MB)...\n",
      "Skipping transformer_blocks.6.attn.to_out.1 (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.6.attn.to_add_out (18.01 MB)...\n",
      "Skipping transformer_blocks.6.attn.norm_added_q (0.00 MB) - below threshold\n",
      "Skipping transformer_blocks.6.attn.norm_added_k (0.00 MB) - below threshold\n",
      "Skipping transformer_blocks.6.norm2 (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.6.ff (144.03 MB)...\n",
      "Quantizing transformer_blocks.6.ff.net (144.03 MB)...\n",
      "Quantizing transformer_blocks.6.ff.net.0 (72.02 MB)...\n",
      "Quantizing transformer_blocks.6.ff.net.0.proj (72.02 MB)...\n",
      "Skipping transformer_blocks.6.ff.net.1 (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.6.ff.net.2 (72.01 MB)...\n",
      "Skipping transformer_blocks.6.norm2_context (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.6.ff_context (144.03 MB)...\n",
      "Quantizing transformer_blocks.6.ff_context.net (144.03 MB)...\n",
      "Quantizing transformer_blocks.6.ff_context.net.0 (72.02 MB)...\n",
      "Quantizing transformer_blocks.6.ff_context.net.0.proj (72.02 MB)...\n",
      "Skipping transformer_blocks.6.ff_context.net.1 (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.6.ff_context.net.2 (72.01 MB)...\n",
      "Quantizing transformer_blocks.7 (648.18 MB)...\n",
      "Quantizing transformer_blocks.7.norm1 (108.04 MB)...\n",
      "Skipping transformer_blocks.7.norm1.silu (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.7.norm1.linear (108.04 MB)...\n",
      "Skipping transformer_blocks.7.norm1.norm (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.7.norm1_context (108.04 MB)...\n",
      "Skipping transformer_blocks.7.norm1_context.silu (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.7.norm1_context.linear (108.04 MB)...\n",
      "Skipping transformer_blocks.7.norm1_context.norm (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.7.attn (144.05 MB)...\n",
      "Skipping transformer_blocks.7.attn.norm_q (0.00 MB) - below threshold\n",
      "Skipping transformer_blocks.7.attn.norm_k (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.7.attn.to_q (18.01 MB)...\n",
      "Quantizing transformer_blocks.7.attn.to_k (18.01 MB)...\n",
      "Quantizing transformer_blocks.7.attn.to_v (18.01 MB)...\n",
      "Quantizing transformer_blocks.7.attn.add_k_proj (18.01 MB)...\n",
      "Quantizing transformer_blocks.7.attn.add_v_proj (18.01 MB)...\n",
      "Quantizing transformer_blocks.7.attn.add_q_proj (18.01 MB)...\n",
      "Quantizing transformer_blocks.7.attn.to_out (18.01 MB)...\n",
      "Quantizing transformer_blocks.7.attn.to_out.0 (18.01 MB)...\n",
      "Skipping transformer_blocks.7.attn.to_out.1 (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.7.attn.to_add_out (18.01 MB)...\n",
      "Skipping transformer_blocks.7.attn.norm_added_q (0.00 MB) - below threshold\n",
      "Skipping transformer_blocks.7.attn.norm_added_k (0.00 MB) - below threshold\n",
      "Skipping transformer_blocks.7.norm2 (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.7.ff (144.03 MB)...\n",
      "Quantizing transformer_blocks.7.ff.net (144.03 MB)...\n",
      "Quantizing transformer_blocks.7.ff.net.0 (72.02 MB)...\n",
      "Quantizing transformer_blocks.7.ff.net.0.proj (72.02 MB)...\n",
      "Skipping transformer_blocks.7.ff.net.1 (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.7.ff.net.2 (72.01 MB)...\n",
      "Skipping transformer_blocks.7.norm2_context (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.7.ff_context (144.03 MB)...\n",
      "Quantizing transformer_blocks.7.ff_context.net (144.03 MB)...\n",
      "Quantizing transformer_blocks.7.ff_context.net.0 (72.02 MB)...\n",
      "Quantizing transformer_blocks.7.ff_context.net.0.proj (72.02 MB)...\n",
      "Skipping transformer_blocks.7.ff_context.net.1 (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.7.ff_context.net.2 (72.01 MB)...\n",
      "Quantizing transformer_blocks.8 (648.18 MB)...\n",
      "Quantizing transformer_blocks.8.norm1 (108.04 MB)...\n",
      "Skipping transformer_blocks.8.norm1.silu (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.8.norm1.linear (108.04 MB)...\n",
      "Skipping transformer_blocks.8.norm1.norm (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.8.norm1_context (108.04 MB)...\n",
      "Skipping transformer_blocks.8.norm1_context.silu (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.8.norm1_context.linear (108.04 MB)...\n",
      "Skipping transformer_blocks.8.norm1_context.norm (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.8.attn (144.05 MB)...\n",
      "Skipping transformer_blocks.8.attn.norm_q (0.00 MB) - below threshold\n",
      "Skipping transformer_blocks.8.attn.norm_k (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.8.attn.to_q (18.01 MB)...\n",
      "Quantizing transformer_blocks.8.attn.to_k (18.01 MB)...\n",
      "Quantizing transformer_blocks.8.attn.to_v (18.01 MB)...\n",
      "Quantizing transformer_blocks.8.attn.add_k_proj (18.01 MB)...\n",
      "Quantizing transformer_blocks.8.attn.add_v_proj (18.01 MB)...\n",
      "Quantizing transformer_blocks.8.attn.add_q_proj (18.01 MB)...\n",
      "Quantizing transformer_blocks.8.attn.to_out (18.01 MB)...\n",
      "Quantizing transformer_blocks.8.attn.to_out.0 (18.01 MB)...\n",
      "Skipping transformer_blocks.8.attn.to_out.1 (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.8.attn.to_add_out (18.01 MB)...\n",
      "Skipping transformer_blocks.8.attn.norm_added_q (0.00 MB) - below threshold\n",
      "Skipping transformer_blocks.8.attn.norm_added_k (0.00 MB) - below threshold\n",
      "Skipping transformer_blocks.8.norm2 (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.8.ff (144.03 MB)...\n",
      "Quantizing transformer_blocks.8.ff.net (144.03 MB)...\n",
      "Quantizing transformer_blocks.8.ff.net.0 (72.02 MB)...\n",
      "Quantizing transformer_blocks.8.ff.net.0.proj (72.02 MB)...\n",
      "Skipping transformer_blocks.8.ff.net.1 (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.8.ff.net.2 (72.01 MB)...\n",
      "Skipping transformer_blocks.8.norm2_context (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.8.ff_context (144.03 MB)...\n",
      "Quantizing transformer_blocks.8.ff_context.net (144.03 MB)...\n",
      "Quantizing transformer_blocks.8.ff_context.net.0 (72.02 MB)...\n",
      "Quantizing transformer_blocks.8.ff_context.net.0.proj (72.02 MB)...\n",
      "Skipping transformer_blocks.8.ff_context.net.1 (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.8.ff_context.net.2 (72.01 MB)...\n",
      "Quantizing transformer_blocks.9 (648.18 MB)...\n",
      "Quantizing transformer_blocks.9.norm1 (108.04 MB)...\n",
      "Skipping transformer_blocks.9.norm1.silu (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.9.norm1.linear (108.04 MB)...\n",
      "Skipping transformer_blocks.9.norm1.norm (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.9.norm1_context (108.04 MB)...\n",
      "Skipping transformer_blocks.9.norm1_context.silu (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.9.norm1_context.linear (108.04 MB)...\n",
      "Skipping transformer_blocks.9.norm1_context.norm (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.9.attn (144.05 MB)...\n",
      "Skipping transformer_blocks.9.attn.norm_q (0.00 MB) - below threshold\n",
      "Skipping transformer_blocks.9.attn.norm_k (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.9.attn.to_q (18.01 MB)...\n",
      "Quantizing transformer_blocks.9.attn.to_k (18.01 MB)...\n",
      "Quantizing transformer_blocks.9.attn.to_v (18.01 MB)...\n",
      "Quantizing transformer_blocks.9.attn.add_k_proj (18.01 MB)...\n",
      "Quantizing transformer_blocks.9.attn.add_v_proj (18.01 MB)...\n",
      "Quantizing transformer_blocks.9.attn.add_q_proj (18.01 MB)...\n",
      "Quantizing transformer_blocks.9.attn.to_out (18.01 MB)...\n",
      "Quantizing transformer_blocks.9.attn.to_out.0 (18.01 MB)...\n",
      "Skipping transformer_blocks.9.attn.to_out.1 (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.9.attn.to_add_out (18.01 MB)...\n",
      "Skipping transformer_blocks.9.attn.norm_added_q (0.00 MB) - below threshold\n",
      "Skipping transformer_blocks.9.attn.norm_added_k (0.00 MB) - below threshold\n",
      "Skipping transformer_blocks.9.norm2 (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.9.ff (144.03 MB)...\n",
      "Quantizing transformer_blocks.9.ff.net (144.03 MB)...\n",
      "Quantizing transformer_blocks.9.ff.net.0 (72.02 MB)...\n",
      "Quantizing transformer_blocks.9.ff.net.0.proj (72.02 MB)...\n",
      "Skipping transformer_blocks.9.ff.net.1 (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.9.ff.net.2 (72.01 MB)...\n",
      "Skipping transformer_blocks.9.norm2_context (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.9.ff_context (144.03 MB)...\n",
      "Quantizing transformer_blocks.9.ff_context.net (144.03 MB)...\n",
      "Quantizing transformer_blocks.9.ff_context.net.0 (72.02 MB)...\n",
      "Quantizing transformer_blocks.9.ff_context.net.0.proj (72.02 MB)...\n",
      "Skipping transformer_blocks.9.ff_context.net.1 (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.9.ff_context.net.2 (72.01 MB)...\n",
      "Quantizing transformer_blocks.10 (648.18 MB)...\n",
      "Quantizing transformer_blocks.10.norm1 (108.04 MB)...\n",
      "Skipping transformer_blocks.10.norm1.silu (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.10.norm1.linear (108.04 MB)...\n",
      "Skipping transformer_blocks.10.norm1.norm (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.10.norm1_context (108.04 MB)...\n",
      "Skipping transformer_blocks.10.norm1_context.silu (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.10.norm1_context.linear (108.04 MB)...\n",
      "Skipping transformer_blocks.10.norm1_context.norm (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.10.attn (144.05 MB)...\n",
      "Skipping transformer_blocks.10.attn.norm_q (0.00 MB) - below threshold\n",
      "Skipping transformer_blocks.10.attn.norm_k (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.10.attn.to_q (18.01 MB)...\n",
      "Quantizing transformer_blocks.10.attn.to_k (18.01 MB)...\n",
      "Quantizing transformer_blocks.10.attn.to_v (18.01 MB)...\n",
      "Quantizing transformer_blocks.10.attn.add_k_proj (18.01 MB)...\n",
      "Quantizing transformer_blocks.10.attn.add_v_proj (18.01 MB)...\n",
      "Quantizing transformer_blocks.10.attn.add_q_proj (18.01 MB)...\n",
      "Quantizing transformer_blocks.10.attn.to_out (18.01 MB)...\n",
      "Quantizing transformer_blocks.10.attn.to_out.0 (18.01 MB)...\n",
      "Skipping transformer_blocks.10.attn.to_out.1 (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.10.attn.to_add_out (18.01 MB)...\n",
      "Skipping transformer_blocks.10.attn.norm_added_q (0.00 MB) - below threshold\n",
      "Skipping transformer_blocks.10.attn.norm_added_k (0.00 MB) - below threshold\n",
      "Skipping transformer_blocks.10.norm2 (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.10.ff (144.03 MB)...\n",
      "Quantizing transformer_blocks.10.ff.net (144.03 MB)...\n",
      "Quantizing transformer_blocks.10.ff.net.0 (72.02 MB)...\n",
      "Quantizing transformer_blocks.10.ff.net.0.proj (72.02 MB)...\n",
      "Skipping transformer_blocks.10.ff.net.1 (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.10.ff.net.2 (72.01 MB)...\n",
      "Skipping transformer_blocks.10.norm2_context (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.10.ff_context (144.03 MB)...\n",
      "Quantizing transformer_blocks.10.ff_context.net (144.03 MB)...\n",
      "Quantizing transformer_blocks.10.ff_context.net.0 (72.02 MB)...\n",
      "Quantizing transformer_blocks.10.ff_context.net.0.proj (72.02 MB)...\n",
      "Skipping transformer_blocks.10.ff_context.net.1 (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.10.ff_context.net.2 (72.01 MB)...\n",
      "Quantizing transformer_blocks.11 (648.18 MB)...\n",
      "Quantizing transformer_blocks.11.norm1 (108.04 MB)...\n",
      "Skipping transformer_blocks.11.norm1.silu (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.11.norm1.linear (108.04 MB)...\n",
      "Skipping transformer_blocks.11.norm1.norm (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.11.norm1_context (108.04 MB)...\n",
      "Skipping transformer_blocks.11.norm1_context.silu (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.11.norm1_context.linear (108.04 MB)...\n",
      "Skipping transformer_blocks.11.norm1_context.norm (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.11.attn (144.05 MB)...\n",
      "Skipping transformer_blocks.11.attn.norm_q (0.00 MB) - below threshold\n",
      "Skipping transformer_blocks.11.attn.norm_k (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.11.attn.to_q (18.01 MB)...\n",
      "Quantizing transformer_blocks.11.attn.to_k (18.01 MB)...\n",
      "Quantizing transformer_blocks.11.attn.to_v (18.01 MB)...\n",
      "Quantizing transformer_blocks.11.attn.add_k_proj (18.01 MB)...\n",
      "Quantizing transformer_blocks.11.attn.add_v_proj (18.01 MB)...\n",
      "Quantizing transformer_blocks.11.attn.add_q_proj (18.01 MB)...\n",
      "Quantizing transformer_blocks.11.attn.to_out (18.01 MB)...\n",
      "Quantizing transformer_blocks.11.attn.to_out.0 (18.01 MB)...\n",
      "Skipping transformer_blocks.11.attn.to_out.1 (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.11.attn.to_add_out (18.01 MB)...\n",
      "Skipping transformer_blocks.11.attn.norm_added_q (0.00 MB) - below threshold\n",
      "Skipping transformer_blocks.11.attn.norm_added_k (0.00 MB) - below threshold\n",
      "Skipping transformer_blocks.11.norm2 (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.11.ff (144.03 MB)...\n",
      "Quantizing transformer_blocks.11.ff.net (144.03 MB)...\n",
      "Quantizing transformer_blocks.11.ff.net.0 (72.02 MB)...\n",
      "Quantizing transformer_blocks.11.ff.net.0.proj (72.02 MB)...\n",
      "Skipping transformer_blocks.11.ff.net.1 (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.11.ff.net.2 (72.01 MB)...\n",
      "Skipping transformer_blocks.11.norm2_context (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.11.ff_context (144.03 MB)...\n",
      "Quantizing transformer_blocks.11.ff_context.net (144.03 MB)...\n",
      "Quantizing transformer_blocks.11.ff_context.net.0 (72.02 MB)...\n",
      "Quantizing transformer_blocks.11.ff_context.net.0.proj (72.02 MB)...\n",
      "Skipping transformer_blocks.11.ff_context.net.1 (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.11.ff_context.net.2 (72.01 MB)...\n",
      "Quantizing transformer_blocks.12 (648.18 MB)...\n",
      "Quantizing transformer_blocks.12.norm1 (108.04 MB)...\n",
      "Skipping transformer_blocks.12.norm1.silu (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.12.norm1.linear (108.04 MB)...\n",
      "Skipping transformer_blocks.12.norm1.norm (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.12.norm1_context (108.04 MB)...\n",
      "Skipping transformer_blocks.12.norm1_context.silu (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.12.norm1_context.linear (108.04 MB)...\n",
      "Skipping transformer_blocks.12.norm1_context.norm (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.12.attn (144.05 MB)...\n",
      "Skipping transformer_blocks.12.attn.norm_q (0.00 MB) - below threshold\n",
      "Skipping transformer_blocks.12.attn.norm_k (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.12.attn.to_q (18.01 MB)...\n",
      "Quantizing transformer_blocks.12.attn.to_k (18.01 MB)...\n",
      "Quantizing transformer_blocks.12.attn.to_v (18.01 MB)...\n",
      "Quantizing transformer_blocks.12.attn.add_k_proj (18.01 MB)...\n",
      "Quantizing transformer_blocks.12.attn.add_v_proj (18.01 MB)...\n",
      "Quantizing transformer_blocks.12.attn.add_q_proj (18.01 MB)...\n",
      "Quantizing transformer_blocks.12.attn.to_out (18.01 MB)...\n",
      "Quantizing transformer_blocks.12.attn.to_out.0 (18.01 MB)...\n",
      "Skipping transformer_blocks.12.attn.to_out.1 (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.12.attn.to_add_out (18.01 MB)...\n",
      "Skipping transformer_blocks.12.attn.norm_added_q (0.00 MB) - below threshold\n",
      "Skipping transformer_blocks.12.attn.norm_added_k (0.00 MB) - below threshold\n",
      "Skipping transformer_blocks.12.norm2 (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.12.ff (144.03 MB)...\n",
      "Quantizing transformer_blocks.12.ff.net (144.03 MB)...\n",
      "Quantizing transformer_blocks.12.ff.net.0 (72.02 MB)...\n",
      "Quantizing transformer_blocks.12.ff.net.0.proj (72.02 MB)...\n",
      "Skipping transformer_blocks.12.ff.net.1 (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.12.ff.net.2 (72.01 MB)...\n",
      "Skipping transformer_blocks.12.norm2_context (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.12.ff_context (144.03 MB)...\n",
      "Quantizing transformer_blocks.12.ff_context.net (144.03 MB)...\n",
      "Quantizing transformer_blocks.12.ff_context.net.0 (72.02 MB)...\n",
      "Quantizing transformer_blocks.12.ff_context.net.0.proj (72.02 MB)...\n",
      "Skipping transformer_blocks.12.ff_context.net.1 (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.12.ff_context.net.2 (72.01 MB)...\n",
      "Quantizing transformer_blocks.13 (648.18 MB)...\n",
      "Quantizing transformer_blocks.13.norm1 (108.04 MB)...\n",
      "Skipping transformer_blocks.13.norm1.silu (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.13.norm1.linear (108.04 MB)...\n",
      "Skipping transformer_blocks.13.norm1.norm (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.13.norm1_context (108.04 MB)...\n",
      "Skipping transformer_blocks.13.norm1_context.silu (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.13.norm1_context.linear (108.04 MB)...\n",
      "Skipping transformer_blocks.13.norm1_context.norm (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.13.attn (144.05 MB)...\n",
      "Skipping transformer_blocks.13.attn.norm_q (0.00 MB) - below threshold\n",
      "Skipping transformer_blocks.13.attn.norm_k (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.13.attn.to_q (18.01 MB)...\n",
      "Quantizing transformer_blocks.13.attn.to_k (18.01 MB)...\n",
      "Quantizing transformer_blocks.13.attn.to_v (18.01 MB)...\n",
      "Quantizing transformer_blocks.13.attn.add_k_proj (18.01 MB)...\n",
      "Quantizing transformer_blocks.13.attn.add_v_proj (18.01 MB)...\n",
      "Quantizing transformer_blocks.13.attn.add_q_proj (18.01 MB)...\n",
      "Quantizing transformer_blocks.13.attn.to_out (18.01 MB)...\n",
      "Quantizing transformer_blocks.13.attn.to_out.0 (18.01 MB)...\n",
      "Skipping transformer_blocks.13.attn.to_out.1 (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.13.attn.to_add_out (18.01 MB)...\n",
      "Skipping transformer_blocks.13.attn.norm_added_q (0.00 MB) - below threshold\n",
      "Skipping transformer_blocks.13.attn.norm_added_k (0.00 MB) - below threshold\n",
      "Skipping transformer_blocks.13.norm2 (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.13.ff (144.03 MB)...\n",
      "Quantizing transformer_blocks.13.ff.net (144.03 MB)...\n",
      "Quantizing transformer_blocks.13.ff.net.0 (72.02 MB)...\n",
      "Quantizing transformer_blocks.13.ff.net.0.proj (72.02 MB)...\n",
      "Skipping transformer_blocks.13.ff.net.1 (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.13.ff.net.2 (72.01 MB)...\n",
      "Skipping transformer_blocks.13.norm2_context (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.13.ff_context (144.03 MB)...\n",
      "Quantizing transformer_blocks.13.ff_context.net (144.03 MB)...\n",
      "Quantizing transformer_blocks.13.ff_context.net.0 (72.02 MB)...\n",
      "Quantizing transformer_blocks.13.ff_context.net.0.proj (72.02 MB)...\n",
      "Skipping transformer_blocks.13.ff_context.net.1 (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.13.ff_context.net.2 (72.01 MB)...\n",
      "Quantizing transformer_blocks.14 (648.18 MB)...\n",
      "Quantizing transformer_blocks.14.norm1 (108.04 MB)...\n",
      "Skipping transformer_blocks.14.norm1.silu (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.14.norm1.linear (108.04 MB)...\n",
      "Skipping transformer_blocks.14.norm1.norm (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.14.norm1_context (108.04 MB)...\n",
      "Skipping transformer_blocks.14.norm1_context.silu (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.14.norm1_context.linear (108.04 MB)...\n",
      "Skipping transformer_blocks.14.norm1_context.norm (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.14.attn (144.05 MB)...\n",
      "Skipping transformer_blocks.14.attn.norm_q (0.00 MB) - below threshold\n",
      "Skipping transformer_blocks.14.attn.norm_k (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.14.attn.to_q (18.01 MB)...\n",
      "Quantizing transformer_blocks.14.attn.to_k (18.01 MB)...\n",
      "Quantizing transformer_blocks.14.attn.to_v (18.01 MB)...\n",
      "Quantizing transformer_blocks.14.attn.add_k_proj (18.01 MB)...\n",
      "Quantizing transformer_blocks.14.attn.add_v_proj (18.01 MB)...\n",
      "Quantizing transformer_blocks.14.attn.add_q_proj (18.01 MB)...\n",
      "Quantizing transformer_blocks.14.attn.to_out (18.01 MB)...\n",
      "Quantizing transformer_blocks.14.attn.to_out.0 (18.01 MB)...\n",
      "Skipping transformer_blocks.14.attn.to_out.1 (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.14.attn.to_add_out (18.01 MB)...\n",
      "Skipping transformer_blocks.14.attn.norm_added_q (0.00 MB) - below threshold\n",
      "Skipping transformer_blocks.14.attn.norm_added_k (0.00 MB) - below threshold\n",
      "Skipping transformer_blocks.14.norm2 (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.14.ff (144.03 MB)...\n",
      "Quantizing transformer_blocks.14.ff.net (144.03 MB)...\n",
      "Quantizing transformer_blocks.14.ff.net.0 (72.02 MB)...\n",
      "Quantizing transformer_blocks.14.ff.net.0.proj (72.02 MB)...\n",
      "Skipping transformer_blocks.14.ff.net.1 (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.14.ff.net.2 (72.01 MB)...\n",
      "Skipping transformer_blocks.14.norm2_context (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.14.ff_context (144.03 MB)...\n",
      "Quantizing transformer_blocks.14.ff_context.net (144.03 MB)...\n",
      "Quantizing transformer_blocks.14.ff_context.net.0 (72.02 MB)...\n",
      "Quantizing transformer_blocks.14.ff_context.net.0.proj (72.02 MB)...\n",
      "Skipping transformer_blocks.14.ff_context.net.1 (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.14.ff_context.net.2 (72.01 MB)...\n",
      "Quantizing transformer_blocks.15 (648.18 MB)...\n",
      "Quantizing transformer_blocks.15.norm1 (108.04 MB)...\n",
      "Skipping transformer_blocks.15.norm1.silu (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.15.norm1.linear (108.04 MB)...\n",
      "Skipping transformer_blocks.15.norm1.norm (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.15.norm1_context (108.04 MB)...\n",
      "Skipping transformer_blocks.15.norm1_context.silu (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.15.norm1_context.linear (108.04 MB)...\n",
      "Skipping transformer_blocks.15.norm1_context.norm (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.15.attn (144.05 MB)...\n",
      "Skipping transformer_blocks.15.attn.norm_q (0.00 MB) - below threshold\n",
      "Skipping transformer_blocks.15.attn.norm_k (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.15.attn.to_q (18.01 MB)...\n",
      "Quantizing transformer_blocks.15.attn.to_k (18.01 MB)...\n",
      "Quantizing transformer_blocks.15.attn.to_v (18.01 MB)...\n",
      "Quantizing transformer_blocks.15.attn.add_k_proj (18.01 MB)...\n",
      "Quantizing transformer_blocks.15.attn.add_v_proj (18.01 MB)...\n",
      "Quantizing transformer_blocks.15.attn.add_q_proj (18.01 MB)...\n",
      "Quantizing transformer_blocks.15.attn.to_out (18.01 MB)...\n",
      "Quantizing transformer_blocks.15.attn.to_out.0 (18.01 MB)...\n",
      "Skipping transformer_blocks.15.attn.to_out.1 (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.15.attn.to_add_out (18.01 MB)...\n",
      "Skipping transformer_blocks.15.attn.norm_added_q (0.00 MB) - below threshold\n",
      "Skipping transformer_blocks.15.attn.norm_added_k (0.00 MB) - below threshold\n",
      "Skipping transformer_blocks.15.norm2 (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.15.ff (144.03 MB)...\n",
      "Quantizing transformer_blocks.15.ff.net (144.03 MB)...\n",
      "Quantizing transformer_blocks.15.ff.net.0 (72.02 MB)...\n",
      "Quantizing transformer_blocks.15.ff.net.0.proj (72.02 MB)...\n",
      "Skipping transformer_blocks.15.ff.net.1 (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.15.ff.net.2 (72.01 MB)...\n",
      "Skipping transformer_blocks.15.norm2_context (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.15.ff_context (144.03 MB)...\n",
      "Quantizing transformer_blocks.15.ff_context.net (144.03 MB)...\n",
      "Quantizing transformer_blocks.15.ff_context.net.0 (72.02 MB)...\n",
      "Quantizing transformer_blocks.15.ff_context.net.0.proj (72.02 MB)...\n",
      "Skipping transformer_blocks.15.ff_context.net.1 (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.15.ff_context.net.2 (72.01 MB)...\n",
      "Quantizing transformer_blocks.16 (648.18 MB)...\n",
      "Quantizing transformer_blocks.16.norm1 (108.04 MB)...\n",
      "Skipping transformer_blocks.16.norm1.silu (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.16.norm1.linear (108.04 MB)...\n",
      "Skipping transformer_blocks.16.norm1.norm (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.16.norm1_context (108.04 MB)...\n",
      "Skipping transformer_blocks.16.norm1_context.silu (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.16.norm1_context.linear (108.04 MB)...\n",
      "Skipping transformer_blocks.16.norm1_context.norm (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.16.attn (144.05 MB)...\n",
      "Skipping transformer_blocks.16.attn.norm_q (0.00 MB) - below threshold\n",
      "Skipping transformer_blocks.16.attn.norm_k (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.16.attn.to_q (18.01 MB)...\n",
      "Quantizing transformer_blocks.16.attn.to_k (18.01 MB)...\n",
      "Quantizing transformer_blocks.16.attn.to_v (18.01 MB)...\n",
      "Quantizing transformer_blocks.16.attn.add_k_proj (18.01 MB)...\n",
      "Quantizing transformer_blocks.16.attn.add_v_proj (18.01 MB)...\n",
      "Quantizing transformer_blocks.16.attn.add_q_proj (18.01 MB)...\n",
      "Quantizing transformer_blocks.16.attn.to_out (18.01 MB)...\n",
      "Quantizing transformer_blocks.16.attn.to_out.0 (18.01 MB)...\n",
      "Skipping transformer_blocks.16.attn.to_out.1 (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.16.attn.to_add_out (18.01 MB)...\n",
      "Skipping transformer_blocks.16.attn.norm_added_q (0.00 MB) - below threshold\n",
      "Skipping transformer_blocks.16.attn.norm_added_k (0.00 MB) - below threshold\n",
      "Skipping transformer_blocks.16.norm2 (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.16.ff (144.03 MB)...\n",
      "Quantizing transformer_blocks.16.ff.net (144.03 MB)...\n",
      "Quantizing transformer_blocks.16.ff.net.0 (72.02 MB)...\n",
      "Quantizing transformer_blocks.16.ff.net.0.proj (72.02 MB)...\n",
      "Skipping transformer_blocks.16.ff.net.1 (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.16.ff.net.2 (72.01 MB)...\n",
      "Skipping transformer_blocks.16.norm2_context (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.16.ff_context (144.03 MB)...\n",
      "Quantizing transformer_blocks.16.ff_context.net (144.03 MB)...\n",
      "Quantizing transformer_blocks.16.ff_context.net.0 (72.02 MB)...\n",
      "Quantizing transformer_blocks.16.ff_context.net.0.proj (72.02 MB)...\n",
      "Skipping transformer_blocks.16.ff_context.net.1 (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.16.ff_context.net.2 (72.01 MB)...\n",
      "Quantizing transformer_blocks.17 (648.18 MB)...\n",
      "Quantizing transformer_blocks.17.norm1 (108.04 MB)...\n",
      "Skipping transformer_blocks.17.norm1.silu (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.17.norm1.linear (108.04 MB)...\n",
      "Skipping transformer_blocks.17.norm1.norm (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.17.norm1_context (108.04 MB)...\n",
      "Skipping transformer_blocks.17.norm1_context.silu (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.17.norm1_context.linear (108.04 MB)...\n",
      "Skipping transformer_blocks.17.norm1_context.norm (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.17.attn (144.05 MB)...\n",
      "Skipping transformer_blocks.17.attn.norm_q (0.00 MB) - below threshold\n",
      "Skipping transformer_blocks.17.attn.norm_k (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.17.attn.to_q (18.01 MB)...\n",
      "Quantizing transformer_blocks.17.attn.to_k (18.01 MB)...\n",
      "Quantizing transformer_blocks.17.attn.to_v (18.01 MB)...\n",
      "Quantizing transformer_blocks.17.attn.add_k_proj (18.01 MB)...\n",
      "Quantizing transformer_blocks.17.attn.add_v_proj (18.01 MB)...\n",
      "Quantizing transformer_blocks.17.attn.add_q_proj (18.01 MB)...\n",
      "Quantizing transformer_blocks.17.attn.to_out (18.01 MB)...\n",
      "Quantizing transformer_blocks.17.attn.to_out.0 (18.01 MB)...\n",
      "Skipping transformer_blocks.17.attn.to_out.1 (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.17.attn.to_add_out (18.01 MB)...\n",
      "Skipping transformer_blocks.17.attn.norm_added_q (0.00 MB) - below threshold\n",
      "Skipping transformer_blocks.17.attn.norm_added_k (0.00 MB) - below threshold\n",
      "Skipping transformer_blocks.17.norm2 (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.17.ff (144.03 MB)...\n",
      "Quantizing transformer_blocks.17.ff.net (144.03 MB)...\n",
      "Quantizing transformer_blocks.17.ff.net.0 (72.02 MB)...\n",
      "Quantizing transformer_blocks.17.ff.net.0.proj (72.02 MB)...\n",
      "Skipping transformer_blocks.17.ff.net.1 (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.17.ff.net.2 (72.01 MB)...\n",
      "Skipping transformer_blocks.17.norm2_context (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.17.ff_context (144.03 MB)...\n",
      "Quantizing transformer_blocks.17.ff_context.net (144.03 MB)...\n",
      "Quantizing transformer_blocks.17.ff_context.net.0 (72.02 MB)...\n",
      "Quantizing transformer_blocks.17.ff_context.net.0.proj (72.02 MB)...\n",
      "Skipping transformer_blocks.17.ff_context.net.1 (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.17.ff_context.net.2 (72.01 MB)...\n",
      "Quantizing transformer_blocks.18 (648.18 MB)...\n",
      "Quantizing transformer_blocks.18.norm1 (108.04 MB)...\n",
      "Skipping transformer_blocks.18.norm1.silu (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.18.norm1.linear (108.04 MB)...\n",
      "Skipping transformer_blocks.18.norm1.norm (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.18.norm1_context (108.04 MB)...\n",
      "Skipping transformer_blocks.18.norm1_context.silu (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.18.norm1_context.linear (108.04 MB)...\n",
      "Skipping transformer_blocks.18.norm1_context.norm (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.18.attn (144.05 MB)...\n",
      "Skipping transformer_blocks.18.attn.norm_q (0.00 MB) - below threshold\n",
      "Skipping transformer_blocks.18.attn.norm_k (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.18.attn.to_q (18.01 MB)...\n",
      "Quantizing transformer_blocks.18.attn.to_k (18.01 MB)...\n",
      "Quantizing transformer_blocks.18.attn.to_v (18.01 MB)...\n",
      "Quantizing transformer_blocks.18.attn.add_k_proj (18.01 MB)...\n",
      "Quantizing transformer_blocks.18.attn.add_v_proj (18.01 MB)...\n",
      "Quantizing transformer_blocks.18.attn.add_q_proj (18.01 MB)...\n",
      "Quantizing transformer_blocks.18.attn.to_out (18.01 MB)...\n",
      "Quantizing transformer_blocks.18.attn.to_out.0 (18.01 MB)...\n",
      "Skipping transformer_blocks.18.attn.to_out.1 (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.18.attn.to_add_out (18.01 MB)...\n",
      "Skipping transformer_blocks.18.attn.norm_added_q (0.00 MB) - below threshold\n",
      "Skipping transformer_blocks.18.attn.norm_added_k (0.00 MB) - below threshold\n",
      "Skipping transformer_blocks.18.norm2 (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.18.ff (144.03 MB)...\n",
      "Quantizing transformer_blocks.18.ff.net (144.03 MB)...\n",
      "Quantizing transformer_blocks.18.ff.net.0 (72.02 MB)...\n",
      "Quantizing transformer_blocks.18.ff.net.0.proj (72.02 MB)...\n",
      "Skipping transformer_blocks.18.ff.net.1 (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.18.ff.net.2 (72.01 MB)...\n",
      "Skipping transformer_blocks.18.norm2_context (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.18.ff_context (144.03 MB)...\n",
      "Quantizing transformer_blocks.18.ff_context.net (144.03 MB)...\n",
      "Quantizing transformer_blocks.18.ff_context.net.0 (72.02 MB)...\n",
      "Quantizing transformer_blocks.18.ff_context.net.0.proj (72.02 MB)...\n",
      "Skipping transformer_blocks.18.ff_context.net.1 (0.00 MB) - below threshold\n",
      "Quantizing transformer_blocks.18.ff_context.net.2 (72.01 MB)...\n",
      "Quantizing single_transformer_blocks (10262.47 MB)...\n",
      "Quantizing single_transformer_blocks.0 (270.06 MB)...\n",
      "Quantizing single_transformer_blocks.0.norm (54.02 MB)...\n",
      "Skipping single_transformer_blocks.0.norm.silu (0.00 MB) - below threshold\n",
      "Quantizing single_transformer_blocks.0.norm.linear (54.02 MB)...\n",
      "Skipping single_transformer_blocks.0.norm.norm (0.00 MB) - below threshold\n",
      "Quantizing single_transformer_blocks.0.proj_mlp (72.02 MB)...\n",
      "Skipping single_transformer_blocks.0.act_mlp (0.00 MB) - below threshold\n",
      "Quantizing single_transformer_blocks.0.proj_out (90.01 MB)...\n",
      "Quantizing single_transformer_blocks.0.attn (54.02 MB)...\n",
      "Skipping single_transformer_blocks.0.attn.norm_q (0.00 MB) - below threshold\n",
      "Skipping single_transformer_blocks.0.attn.norm_k (0.00 MB) - below threshold\n",
      "Quantizing single_transformer_blocks.0.attn.to_q (18.01 MB)...\n",
      "Quantizing single_transformer_blocks.0.attn.to_k (18.01 MB)...\n",
      "Quantizing single_transformer_blocks.0.attn.to_v (18.01 MB)...\n",
      "Quantizing single_transformer_blocks.1 (270.06 MB)...\n",
      "Quantizing single_transformer_blocks.1.norm (54.02 MB)...\n",
      "Skipping single_transformer_blocks.1.norm.silu (0.00 MB) - below threshold\n",
      "Quantizing single_transformer_blocks.1.norm.linear (54.02 MB)...\n",
      "Skipping single_transformer_blocks.1.norm.norm (0.00 MB) - below threshold\n",
      "Quantizing single_transformer_blocks.1.proj_mlp (72.02 MB)...\n",
      "Skipping single_transformer_blocks.1.act_mlp (0.00 MB) - below threshold\n",
      "Quantizing single_transformer_blocks.1.proj_out (90.01 MB)...\n",
      "Quantizing single_transformer_blocks.1.attn (54.02 MB)...\n",
      "Skipping single_transformer_blocks.1.attn.norm_q (0.00 MB) - below threshold\n",
      "Skipping single_transformer_blocks.1.attn.norm_k (0.00 MB) - below threshold\n",
      "Quantizing single_transformer_blocks.1.attn.to_q (18.01 MB)...\n",
      "Quantizing single_transformer_blocks.1.attn.to_k (18.01 MB)...\n",
      "Quantizing single_transformer_blocks.1.attn.to_v (18.01 MB)...\n",
      "Quantizing single_transformer_blocks.2 (270.06 MB)...\n",
      "Quantizing single_transformer_blocks.2.norm (54.02 MB)...\n",
      "Skipping single_transformer_blocks.2.norm.silu (0.00 MB) - below threshold\n",
      "Quantizing single_transformer_blocks.2.norm.linear (54.02 MB)...\n",
      "Skipping single_transformer_blocks.2.norm.norm (0.00 MB) - below threshold\n",
      "Quantizing single_transformer_blocks.2.proj_mlp (72.02 MB)...\n",
      "Skipping single_transformer_blocks.2.act_mlp (0.00 MB) - below threshold\n",
      "Quantizing single_transformer_blocks.2.proj_out (90.01 MB)...\n",
      "Quantizing single_transformer_blocks.2.attn (54.02 MB)...\n",
      "Skipping single_transformer_blocks.2.attn.norm_q (0.00 MB) - below threshold\n",
      "Skipping single_transformer_blocks.2.attn.norm_k (0.00 MB) - below threshold\n",
      "Quantizing single_transformer_blocks.2.attn.to_q (18.01 MB)...\n",
      "Quantizing single_transformer_blocks.2.attn.to_k (18.01 MB)...\n",
      "Quantizing single_transformer_blocks.2.attn.to_v (18.01 MB)...\n",
      "Quantizing single_transformer_blocks.3 (270.06 MB)...\n",
      "Quantizing single_transformer_blocks.3.norm (54.02 MB)...\n",
      "Skipping single_transformer_blocks.3.norm.silu (0.00 MB) - below threshold\n",
      "Quantizing single_transformer_blocks.3.norm.linear (54.02 MB)...\n",
      "Skipping single_transformer_blocks.3.norm.norm (0.00 MB) - below threshold\n",
      "Quantizing single_transformer_blocks.3.proj_mlp (72.02 MB)...\n",
      "Skipping single_transformer_blocks.3.act_mlp (0.00 MB) - below threshold\n",
      "Quantizing single_transformer_blocks.3.proj_out (90.01 MB)...\n",
      "Quantizing single_transformer_blocks.3.attn (54.02 MB)...\n",
      "Skipping single_transformer_blocks.3.attn.norm_q (0.00 MB) - below threshold\n",
      "Skipping single_transformer_blocks.3.attn.norm_k (0.00 MB) - below threshold\n",
      "Quantizing single_transformer_blocks.3.attn.to_q (18.01 MB)...\n",
      "Quantizing single_transformer_blocks.3.attn.to_k (18.01 MB)...\n",
      "Quantizing single_transformer_blocks.3.attn.to_v (18.01 MB)...\n",
      "Quantizing single_transformer_blocks.4 (270.06 MB)...\n",
      "Quantizing single_transformer_blocks.4.norm (54.02 MB)...\n",
      "Skipping single_transformer_blocks.4.norm.silu (0.00 MB) - below threshold\n",
      "Quantizing single_transformer_blocks.4.norm.linear (54.02 MB)...\n",
      "Skipping single_transformer_blocks.4.norm.norm (0.00 MB) - below threshold\n",
      "Quantizing single_transformer_blocks.4.proj_mlp (72.02 MB)...\n",
      "Skipping single_transformer_blocks.4.act_mlp (0.00 MB) - below threshold\n",
      "Quantizing single_transformer_blocks.4.proj_out (90.01 MB)...\n",
      "Quantizing single_transformer_blocks.4.attn (54.02 MB)...\n",
      "Skipping single_transformer_blocks.4.attn.norm_q (0.00 MB) - below threshold\n",
      "Skipping single_transformer_blocks.4.attn.norm_k (0.00 MB) - below threshold\n",
      "Quantizing single_transformer_blocks.4.attn.to_q (18.01 MB)...\n",
      "Quantizing single_transformer_blocks.4.attn.to_k (18.01 MB)...\n",
      "Quantizing single_transformer_blocks.4.attn.to_v (18.01 MB)...\n",
      "Quantizing single_transformer_blocks.5 (270.06 MB)...\n",
      "Quantizing single_transformer_blocks.5.norm (54.02 MB)...\n",
      "Skipping single_transformer_blocks.5.norm.silu (0.00 MB) - below threshold\n",
      "Quantizing single_transformer_blocks.5.norm.linear (54.02 MB)...\n",
      "Skipping single_transformer_blocks.5.norm.norm (0.00 MB) - below threshold\n",
      "Quantizing single_transformer_blocks.5.proj_mlp (72.02 MB)...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.ao.quantization import quantize_dynamic\n",
    "import gc\n",
    "import numpy as np\n",
    "import torch\n",
    "from diffusers import FluxPipeline\n",
    "\n",
    "pipe = FluxPipeline.from_pretrained(\"black-forest-labs/FLUX.1-schnell\", torch_dtype=torch.bfloat16)\n",
    "\n",
    "def get_model_size(model):\n",
    "    param_size = 0\n",
    "    for param in model.parameters():\n",
    "        param_size += param.nelement() * param.element_size()\n",
    "    buffer_size = 0\n",
    "    for buffer in model.buffers():\n",
    "        buffer_size += buffer.nelement() * buffer.element_size()\n",
    "    size_all_mb = (param_size + buffer_size) / 1024**2\n",
    "    return size_all_mb\n",
    "\n",
    "def check_submodule_sizes(model, parent_name=''):\n",
    "    sizes = []\n",
    "    for name, submodule in model.named_children():\n",
    "        full_name = f\"{parent_name}.{name}\" if parent_name else name\n",
    "        size_mb = get_model_size(submodule)\n",
    "        sizes.append((full_name, size_mb))\n",
    "        sizes.extend(check_submodule_sizes(submodule, full_name))\n",
    "    return sizes\n",
    "\n",
    "def quantize_module(module, dtype=torch.qint8):\n",
    "    return quantize_dynamic(\n",
    "        module,\n",
    "        {torch.nn.Linear},\n",
    "        dtype=dtype\n",
    "    )\n",
    "\n",
    "def recursive_quantize(module, dtype=torch.qint8, size_threshold_mb=10, parent_name=''):\n",
    "    quantized = False\n",
    "    for name, submodule in module.named_children():\n",
    "        full_name = f\"{parent_name}.{name}\" if parent_name else name\n",
    "        size_mb = get_model_size(submodule)\n",
    "        \n",
    "        if size_mb > size_threshold_mb:\n",
    "            print(f\"Quantizing {full_name} ({size_mb:.2f} MB)...\")\n",
    "            if list(submodule.children()):  # If submodule has children\n",
    "                submodule = recursive_quantize(submodule, dtype, size_threshold_mb, full_name)\n",
    "            else:\n",
    "                submodule = quantize_module(submodule, dtype)\n",
    "            setattr(module, name, submodule)\n",
    "            quantized = True\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "        else:\n",
    "            print(f\"Skipping {full_name} ({size_mb:.2f} MB) - below threshold\")\n",
    "    \n",
    "    if quantized:\n",
    "        return module\n",
    "    else:\n",
    "        return quantize_module(module, dtype)\n",
    "\n",
    "def quantize_transformer_recursive(transformer, dtype=torch.qint8, size_threshold_mb=10):\n",
    "    sizes_before = check_submodule_sizes(transformer)\n",
    "    sizes_before.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # print(\"Submodule sizes before quantization:\")\n",
    "    # for name, size in sizes_before:\n",
    "    #     print(f\"{name}: {size:.2f} MB\")\n",
    "    \n",
    "    total_size_before = sum(size for _, size in sizes_before)\n",
    "    print(f\"\\nTotal model size before quantization: {total_size_before:.2f} MB\")\n",
    "    \n",
    "    transformer = recursive_quantize(transformer, dtype, size_threshold_mb)\n",
    "    \n",
    "    sizes_after = check_submodule_sizes(transformer)\n",
    "    total_size_after = sum(size for _, size in sizes_after)\n",
    "    print(f\"\\nTotal model size after quantization: {total_size_after:.2f} MB\")\n",
    "    print(f\"Size reduction: {(1 - total_size_after/total_size_before)*100:.2f}%\")\n",
    "    \n",
    "    return transformer\n",
    "\n",
    "# Assuming 'pipe' is your pipeline object\n",
    "# Move to CPU for quantization\n",
    "pipe.to('cpu')\n",
    "\n",
    "# Clear GPU memory\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "try:\n",
    "    # Recursively check sizes and quantize transformer submodules\n",
    "    pipe.transformer = quantize_transformer_recursive(pipe.transformer, size_threshold_mb=10)\n",
    "    print(\"Quantization successful\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during quantization: {e}\")\n",
    "\n",
    "# Move back to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "pipe.to(device)\n",
    "\n",
    "# Example usage\n",
    "prompt = \"A serene landscape with mountains and a lake\"\n",
    "image = pipe(prompt).images[0]\n",
    "image.save(\"generated_image.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cece28-3a44-4130-b7dd-09afedc01c36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24083267-3f52-4930-a384-557a75db0ce6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c0f2b176-cb02-46f3-a271-f3ef7d26154c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-08T10:16:55.900650Z",
     "iopub.status.busy": "2024-09-08T10:16:55.900650Z",
     "iopub.status.idle": "2024-09-08T10:16:55.905507Z",
     "shell.execute_reply": "2024-09-08T10:16:55.904494Z",
     "shell.execute_reply.started": "2024-09-08T10:16:55.900650Z"
    }
   },
   "source": [
    "## c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e47df5-6fa0-4616-a682-46e61365fff8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "891576ee-0f0d-4f0d-85ec-3f8baab110c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-08T10:56:10.698534Z",
     "iopub.status.busy": "2024-09-08T10:56:10.698534Z",
     "iopub.status.idle": "2024-09-08T10:56:44.395895Z",
     "shell.execute_reply": "2024-09-08T10:56:44.394884Z",
     "shell.execute_reply.started": "2024-09-08T10:56:10.698534Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8c4908e0a2349e38b4a50723f05c8dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rajab\\miniconda3\\envs\\py3_120\\Lib\\site-packages\\transformers\\models\\clip\\modeling_clip.py:480: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b8712a4a9a447828d129fbcbed354d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from diffusers import StableDiffusionPipeline\n",
    "import torch\n",
    "\n",
    "pipe_uq = StableDiffusionPipeline.from_pretrained(\n",
    "    \"stable-diffusion-v1-5/stable-diffusion-v1-5\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "pipe_uq = pipe_uq.to(\"cuda\")\n",
    "\n",
    "prompt = \"a photo of an astronaut riding a horse on mars\"\n",
    "image = pipe_uq(prompt).images[0]  \n",
    "    \n",
    "image.save(\"as_uq1.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8623e960-6aea-4706-b98c-41ebb18ed97c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-08T10:57:40.904608Z",
     "iopub.status.busy": "2024-09-08T10:57:40.904608Z",
     "iopub.status.idle": "2024-09-08T10:58:05.865348Z",
     "shell.execute_reply": "2024-09-08T10:58:05.865348Z",
     "shell.execute_reply.started": "2024-09-08T10:57:40.904608Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc705201f6be45039b2e622edb8dc4b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rajab\\miniconda3\\envs\\py3_120\\Lib\\site-packages\\transformers\\models\\clip\\modeling_clip.py:480: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ead593b553e4a06887b65453bd2b14b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from diffusers import StableDiffusionPipeline\n",
    "import torch\n",
    "\n",
    "pipe_uq = StableDiffusionPipeline.from_pretrained(\n",
    "    \".quantized\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "pipe_uq = pipe_uq.to(\"cuda\")\n",
    "\n",
    "prompt = \"a photo of an astronaut riding a horse on mars\"\n",
    "image = pipe_uq(prompt).images[0]  \n",
    "    \n",
    "image.save(\"as_q1.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a052e9f4-d99a-496b-be40-0dbb3aae3561",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-08T10:45:59.326235Z",
     "iopub.status.busy": "2024-09-08T10:45:59.325245Z",
     "iopub.status.idle": "2024-09-08T10:46:11.251626Z",
     "shell.execute_reply": "2024-09-08T10:46:11.251626Z",
     "shell.execute_reply.started": "2024-09-08T10:45:59.326235Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pipelines loaded with `dtype=torch.float16` cannot run with `cpu` device. It is not recommended to move them to `cpu` as running them will fail. Please make sure to use an accelerator to run the pipeline in inference, due to the lack of support for`float16` operations on this device in PyTorch. Please, remove the `torch_dtype=torch.float16` argument, or use another device for inference.\n",
      "Pipelines loaded with `dtype=torch.float16` cannot run with `cpu` device. It is not recommended to move them to `cpu` as running them will fail. Please make sure to use an accelerator to run the pipeline in inference, due to the lack of support for`float16` operations on this device in PyTorch. Please, remove the `torch_dtype=torch.float16` argument, or use another device for inference.\n",
      "Pipelines loaded with `dtype=torch.float16` cannot run with `cpu` device. It is not recommended to move them to `cpu` as running them will fail. Please make sure to use an accelerator to run the pipeline in inference, due to the lack of support for`float16` operations on this device in PyTorch. Please, remove the `torch_dtype=torch.float16` argument, or use another device for inference.\n",
      "Pipelines loaded with `dtype=torch.float16` cannot run with `cpu` device. It is not recommended to move them to `cpu` as running them will fail. Please make sure to use an accelerator to run the pipeline in inference, due to the lack of support for`float16` operations on this device in PyTorch. Please, remove the `torch_dtype=torch.float16` argument, or use another device for inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total model size before quantization: 917.82 MB\n",
      "Quantizing encoder (65.16 MB)...\n",
      "Skipping encoder.conv_in (0.01 MB) - below threshold\n",
      "Quantizing encoder.down_blocks (45.07 MB)...\n",
      "Skipping encoder.down_blocks.0 (1.41 MB) - below threshold\n",
      "Skipping encoder.down_blocks.1 (5.13 MB) - below threshold\n",
      "Quantizing encoder.down_blocks.2 (20.51 MB)...\n",
      "Quantizing encoder.down_blocks.2.resnets (16.01 MB)...\n",
      "Skipping encoder.down_blocks.2.resnets.0 (7.01 MB) - below threshold\n",
      "Skipping encoder.down_blocks.2.resnets.1 (9.01 MB) - below threshold\n",
      "Skipping encoder.down_blocks.2.downsamplers (4.50 MB) - below threshold\n",
      "Quantizing encoder.down_blocks.3 (18.01 MB)...\n",
      "Quantizing encoder.down_blocks.3.resnets (18.01 MB)...\n",
      "Skipping encoder.down_blocks.3.resnets.0 (9.01 MB) - below threshold\n",
      "Skipping encoder.down_blocks.3.resnets.1 (9.01 MB) - below threshold\n",
      "Quantizing encoder.mid_block (20.02 MB)...\n",
      "Skipping encoder.mid_block.attentions (2.01 MB) - below threshold\n",
      "Quantizing encoder.mid_block.resnets (18.01 MB)...\n",
      "Skipping encoder.mid_block.resnets.0 (9.01 MB) - below threshold\n",
      "Skipping encoder.mid_block.resnets.1 (9.01 MB) - below threshold\n",
      "Skipping encoder.conv_norm_out (0.00 MB) - below threshold\n",
      "Skipping encoder.conv_act (0.00 MB) - below threshold\n",
      "Skipping encoder.conv_out (0.07 MB) - below threshold\n",
      "Quantizing decoder (94.40 MB)...\n",
      "Skipping decoder.conv_in (0.04 MB) - below threshold\n",
      "Quantizing decoder.up_blocks (74.33 MB)...\n",
      "Quantizing decoder.up_blocks.0 (31.52 MB)...\n",
      "Quantizing decoder.up_blocks.0.resnets (27.02 MB)...\n",
      "Skipping decoder.up_blocks.0.resnets.0 (9.01 MB) - below threshold\n",
      "Skipping decoder.up_blocks.0.resnets.1 (9.01 MB) - below threshold\n",
      "Skipping decoder.up_blocks.0.resnets.2 (9.01 MB) - below threshold\n",
      "Skipping decoder.up_blocks.0.upsamplers (4.50 MB) - below threshold\n",
      "Quantizing decoder.up_blocks.1 (31.52 MB)...\n",
      "Quantizing decoder.up_blocks.1.resnets (27.02 MB)...\n",
      "Skipping decoder.up_blocks.1.resnets.0 (9.01 MB) - below threshold\n",
      "Skipping decoder.up_blocks.1.resnets.1 (9.01 MB) - below threshold\n",
      "Skipping decoder.up_blocks.1.resnets.2 (9.01 MB) - below threshold\n",
      "Skipping decoder.up_blocks.1.upsamplers (4.50 MB) - below threshold\n",
      "Skipping decoder.up_blocks.2 (9.26 MB) - below threshold\n",
      "Skipping decoder.up_blocks.3 (2.04 MB) - below threshold\n",
      "Quantizing decoder.mid_block (20.02 MB)...\n",
      "Skipping decoder.mid_block.attentions (2.01 MB) - below threshold\n",
      "Quantizing decoder.mid_block.resnets (18.01 MB)...\n",
      "Skipping decoder.mid_block.resnets.0 (9.01 MB) - below threshold\n",
      "Skipping decoder.mid_block.resnets.1 (9.01 MB) - below threshold\n",
      "Skipping decoder.conv_norm_out (0.00 MB) - below threshold\n",
      "Skipping decoder.conv_act (0.00 MB) - below threshold\n",
      "Skipping decoder.conv_out (0.01 MB) - below threshold\n",
      "Skipping quant_conv (0.00 MB) - below threshold\n",
      "Skipping post_quant_conv (0.00 MB) - below threshold\n",
      "\n",
      "Total model size after quantization: 917.82 MB\n",
      "Size reduction: 0.00%\n",
      "Quantization successful\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9525d77bf86a4484a97489ccbb15dab6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.ao.quantization import quantize_dynamic\n",
    "import gc\n",
    "import numpy as np\n",
    "def get_model_size(model):\n",
    "    param_size = 0\n",
    "    for param in model.parameters():\n",
    "        param_size += param.nelement() * param.element_size()\n",
    "    buffer_size = 0\n",
    "    for buffer in model.buffers():\n",
    "        buffer_size += buffer.nelement() * buffer.element_size()\n",
    "    size_all_mb = (param_size + buffer_size) / 1024**2\n",
    "    return size_all_mb\n",
    "\n",
    "def check_submodule_sizes(model, parent_name=''):\n",
    "    sizes = []\n",
    "    for name, submodule in model.named_children():\n",
    "        full_name = f\"{parent_name}.{name}\" if parent_name else name\n",
    "        size_mb = get_model_size(submodule)\n",
    "        sizes.append((full_name, size_mb))\n",
    "        sizes.extend(check_submodule_sizes(submodule, full_name))\n",
    "    return sizes\n",
    "\n",
    "def quantize_module(module, dtype=torch.qint8):\n",
    "    return quantize_dynamic(\n",
    "        module,\n",
    "        {torch.nn.Linear},\n",
    "        dtype=dtype\n",
    "    )\n",
    "\n",
    "def recursive_quantize(module, dtype=torch.qint8, size_threshold_mb=10, parent_name=''):\n",
    "    quantized = False\n",
    "    for name, submodule in module.named_children():\n",
    "        full_name = f\"{parent_name}.{name}\" if parent_name else name\n",
    "        size_mb = get_model_size(submodule)\n",
    "        \n",
    "        if size_mb > size_threshold_mb:\n",
    "            print(f\"Quantizing {full_name} ({size_mb:.2f} MB)...\")\n",
    "            if list(submodule.children()):  # If submodule has children\n",
    "                submodule = recursive_quantize(submodule, dtype, size_threshold_mb, full_name)\n",
    "            else:\n",
    "                submodule = quantize_module(submodule, dtype)\n",
    "            setattr(module, name, submodule)\n",
    "            quantized = True\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "        else:\n",
    "            print(f\"Skipping {full_name} ({size_mb:.2f} MB) - below threshold\")\n",
    "    \n",
    "    if quantized:\n",
    "        return module\n",
    "    else:\n",
    "        return quantize_module(module, dtype)\n",
    "\n",
    "def quantize_transformer_recursive(transformer, dtype=torch.qint8, size_threshold_mb=10):\n",
    "    sizes_before = check_submodule_sizes(transformer)\n",
    "    sizes_before.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    total_size_before = sum(size for _, size in sizes_before)\n",
    "    print(f\"\\nTotal model size before quantization: {total_size_before:.2f} MB\")\n",
    "    \n",
    "    transformer = recursive_quantize(transformer, dtype, size_threshold_mb)\n",
    "    \n",
    "    sizes_after = check_submodule_sizes(transformer)\n",
    "    total_size_after = sum(size for _, size in sizes_after)\n",
    "    print(f\"\\nTotal model size after quantization: {total_size_after:.2f} MB\")\n",
    "    print(f\"Size reduction: {(1 - total_size_after/total_size_before)*100:.2f}%\")\n",
    "    \n",
    "    return transformer\n",
    "\n",
    "\n",
    "# Clear GPU memory\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "try:\n",
    "    # Recursively check sizes and quantize transformer submodules\n",
    "    pipe.transformer = quantize_transformer_recursive(pipe.vae, size_threshold_mb=10)\n",
    "    print(\"Quantization successful\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during quantization: {e}\")\n",
    "\n",
    "# Move back to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "pipe.to(device)\n",
    "\n",
    "# Example usage\n",
    "prompt = \"A serene landscape with mountains and a lake\"\n",
    "image = pipe(prompt).images[0]\n",
    "image.save(\"generated_image_1.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "046cacb6-d1a2-43b5-9fcf-6f73f36a5360",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-08T10:43:35.530517Z",
     "iopub.status.busy": "2024-09-08T10:43:35.530517Z",
     "iopub.status.idle": "2024-09-08T10:43:35.536026Z",
     "shell.execute_reply": "2024-09-08T10:43:35.535014Z",
     "shell.execute_reply.started": "2024-09-08T10:43:35.530517Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_all_modules_name(pipe):\n",
    "    res = []\n",
    "    for k in pipe.__dict__:\n",
    "        if isinstance(getattr(pipe, k), torch.nn.modules.module.Module):\n",
    "            res.append(k)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d5ba45-dbfa-40e1-8566-542d67313bec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ba7bc3a0-dbc7-4b4c-8472-29531aa59bf5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-08T10:44:08.730212Z",
     "iopub.status.busy": "2024-09-08T10:44:08.730212Z",
     "iopub.status.idle": "2024-09-08T10:44:08.732912Z",
     "shell.execute_reply": "2024-09-08T10:44:08.732912Z",
     "shell.execute_reply.started": "2024-09-08T10:44:08.730212Z"
    }
   },
   "outputs": [],
   "source": [
    "res = get_all_modules_name(pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c58067f1-f195-41e1-9ffe-d70cc67e3a26",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-08T10:45:19.458588Z",
     "iopub.status.busy": "2024-09-08T10:45:19.458588Z",
     "iopub.status.idle": "2024-09-08T10:45:19.470797Z",
     "shell.execute_reply": "2024-09-08T10:45:19.470797Z",
     "shell.execute_reply.started": "2024-09-08T10:45:19.458588Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vae 159.55708122253418\n",
      "text_encoder 234.7198257446289\n",
      "unet 1639.406135559082\n",
      "safety_checker 579.8008270263672\n"
     ]
    }
   ],
   "source": [
    "t = 0\n",
    "for m in res:\n",
    "    s = get_model_size(getattr(pipe, m))\n",
    "    t += s\n",
    "    print(m,s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ea7c9bd1-3c3b-4223-8efc-369e2d2f3cb5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-08T10:51:10.188370Z",
     "iopub.status.busy": "2024-09-08T10:51:10.188370Z",
     "iopub.status.idle": "2024-09-08T10:51:13.207700Z",
     "shell.execute_reply": "2024-09-08T10:51:13.206183Z",
     "shell.execute_reply.started": "2024-09-08T10:51:10.188370Z"
    }
   },
   "outputs": [],
   "source": [
    "pipe.save_pretrained(\".quantized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8f8cb484-e8be-47d2-b23e-d5fe3e777c7c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-08T10:52:37.633708Z",
     "iopub.status.busy": "2024-09-08T10:52:37.632703Z",
     "iopub.status.idle": "2024-09-08T10:52:43.222204Z",
     "shell.execute_reply": "2024-09-08T10:52:43.221663Z",
     "shell.execute_reply.started": "2024-09-08T10:52:37.633708Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6990f45462d4723816d137d59f2ac11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"a photo of an astronaut riding a horse on mars\"\n",
    "image = pipe(prompt).images[0]  \n",
    "    \n",
    "image.save(\"astr_quantized.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c175a0-56fd-4dda-996e-b57dd1694834",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Quantizater:\n",
    "    def __init__(self):\n",
    "        self.type = None\n",
    "    def set_model_name(self, name):\n",
    "        self.type = \"model\"\n",
    "        self.model_name = name\n",
    "    def set_pipeline(self, pipeline):\n",
    "        self.type = \"pipe\"\n",
    "        self.pipeline = pipeline\n",
    "        self.modules = self.get_all_modules_name(pipeline)\n",
    "    def execute_recursively(self):\n",
    "        if self.type == \"pipe\":\n",
    "            for module in self.modules:\n",
    "                self.recursive_quantize(module, )\n",
    "        elif self.type == \"model\":\n",
    "            self.quantize_model()\n",
    "    def execute(self):\n",
    "        pass\n",
    "\n",
    "    def quantize_model(self,use_4bit=True, bnb_4bit_compute_dtype=\"float16\", \n",
    "                       bnb_4bit_quant_type= \"nf4\", use_nested_quant=False ):\n",
    "        compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=use_4bit,\n",
    "            bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "            bnb_4bit_compute_dtype=compute_dtype,\n",
    "            bnb_4bit_use_double_quant=use_nested_quant,\n",
    "        )\n",
    "        \n",
    "        # Check GPU compatibility with bfloat16\n",
    "        if compute_dtype == torch.float16 and use_4bit:\n",
    "            major, _ = torch.cuda.get_device_capability()\n",
    "            if major >= 8:\n",
    "                print(\"=\" * 80)\n",
    "                print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
    "                print(\"=\" * 80)\n",
    "                \n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.model_name,\n",
    "            quantization_config=bnb_config,\n",
    "        )\n",
    "    def get_all_modules_name(self, pipe):\n",
    "        res = []\n",
    "        for k in pipe.__dict__:\n",
    "            if isinstance(getattr(pipe, k), torch.nn.modules.module.Module):\n",
    "                res.append(k)\n",
    "        return res\n",
    "    def get_model_size(self, model):\n",
    "        param_size = 0\n",
    "        for param in model.parameters():\n",
    "            param_size += param.nelement() * param.element_size()\n",
    "        buffer_size = 0\n",
    "        for buffer in model.buffers():\n",
    "            buffer_size += buffer.nelement() * buffer.element_size()\n",
    "        size_all_mb = (param_size + buffer_size) / 1024**2\n",
    "        return size_all_mb\n",
    "    def quantize_module(self, module, dtype=torch.qint8):\n",
    "        return quantize_dynamic(\n",
    "            module,\n",
    "            {torch.nn.Linear},\n",
    "            dtype=dtype\n",
    "        )\n",
    "    def recursive_quantize(self, module, dtype=torch.qint8, size_threshold_mb=10, parent_name=''):\n",
    "        quantized = False\n",
    "        for name, submodule in module.named_children():\n",
    "            full_name = f\"{parent_name}.{name}\" if parent_name else name\n",
    "            size_mb = get_model_size(submodule)\n",
    "            \n",
    "            if size_mb > size_threshold_mb:\n",
    "                print(f\"Quantizing {full_name} ({size_mb:.2f} MB)...\")\n",
    "                if list(submodule.children()):  # If submodule has children\n",
    "                    submodule = self.recursive_quantize(submodule, dtype, size_threshold_mb, full_name)\n",
    "                else:\n",
    "                    submodule = self.quantize_module(submodule, dtype)\n",
    "                setattr(module, name, submodule)\n",
    "                quantized = True\n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache()\n",
    "            else:\n",
    "                print(f\"Skipping {full_name} ({size_mb:.2f} MB) - below threshold\")\n",
    "        \n",
    "        if quantized:\n",
    "            return module\n",
    "        else:\n",
    "            return self.quantize_module(module, dtype)\n",
    "    def save(self, output_path):\n",
    "        if self.type == \"pipe\":\n",
    "            self.pipeline.save_pretrained(output_path)\n",
    "        elif self.type == \"model\":\n",
    "            self.model.save_pretrained(output_path)\n",
    "        else:\n",
    "            print(\"nothing to save\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08c9649-8c8a-4ced-ae98-c93a7e54fab4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-12T13:53:06.210932Z",
     "iopub.status.busy": "2024-09-12T13:53:06.209934Z",
     "iopub.status.idle": "2024-09-12T13:53:06.265185Z",
     "shell.execute_reply": "2024-09-12T13:53:06.259680Z",
     "shell.execute_reply.started": "2024-09-12T13:53:06.210932Z"
    }
   },
   "source": [
    "## d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97b6c4c2-0cf5-4f27-9219-92b2d16bfc6c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-12T14:04:20.833954Z",
     "iopub.status.busy": "2024-09-12T14:04:20.832963Z",
     "iopub.status.idle": "2024-09-12T14:04:20.841644Z",
     "shell.execute_reply": "2024-09-12T14:04:20.840640Z",
     "shell.execute_reply.started": "2024-09-12T14:04:20.833954Z"
    }
   },
   "outputs": [],
   "source": [
    "checkpoint_val = 0\n",
    "import time\n",
    "from time import strftime, localtime\n",
    "checkpoint_prev_time = None\n",
    "def convert_to_readable_format(t):\n",
    "    return strftime('%Y-%m-%d %H:%M:%S', localtime(1347517370))\n",
    "\n",
    "def checkpoint():\n",
    "    global checkpoint_val\n",
    "    if checkpoint_prev_time is None:\n",
    "        checkpoint_prev_time = time.time()\n",
    "        print(checkpoint_val, \"started\", convert_to_readable_format(checkpoint_prev_time))\n",
    "    else:\n",
    "        print(checkpoint_val, \"delta time\", convert_to_readable_format(time.time()  - checkpoint_prev_time))\n",
    "        checkpoint_prev_time = time.time()\n",
    "    checkpoint_val += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "646044ad-dff2-4bd1-bb41-be15315dac8f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-12T14:07:24.270707Z",
     "iopub.status.busy": "2024-09-12T14:07:24.270707Z",
     "iopub.status.idle": "2024-09-12T14:07:24.309504Z",
     "shell.execute_reply": "2024-09-12T14:07:24.307491Z",
     "shell.execute_reply.started": "2024-09-12T14:07:24.270707Z"
    }
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'quanti_' from 'torchao.quantization.quant_api' (C:\\Users\\rajab\\miniconda3\\envs\\py3_120\\Lib\\site-packages\\torchao\\quantization\\quant_api.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdiffusers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FluxTransformer2DModel\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchao\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquantization\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquant_api\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m quanti_, int8_weight_only\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      5\u001b[0m checkpoint()\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'quanti_' from 'torchao.quantization.quant_api' (C:\\Users\\rajab\\miniconda3\\envs\\py3_120\\Lib\\site-packages\\torchao\\quantization\\quant_api.py)"
     ]
    }
   ],
   "source": [
    "from diffusers import FluxTransformer2DModel\n",
    "from torchao.quantization.quant_api import quanti_, int8_weight_only\n",
    "import torch\n",
    "\n",
    "checkpoint()\n",
    "ckpt_id = \"black-forest-labs/FLUX.1-schnell\"\n",
    "\n",
    "transformer = FluxTransformer2DModel.from_pretrained(\n",
    "    ckpt_id, subfolder=\"transformer\", torch_dtype=torch.bfloat16\n",
    ")\n",
    "checkpoint()\n",
    "quantize_(transformer, int8_weight_only())\n",
    "checkpoint()\n",
    "output_dir = \"./flux-schnell-int8wo\"\n",
    "transformer.save_pretrained(output_dir, safe_serialization=False)\n",
    "checkpoint()\n",
    "# Push to the Hub optionally.\n",
    "# save_to = \"sayakpaul/flux-schnell-int8wo\"\n",
    "# transformer.push_to_hub(save_to, safe_serialization=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84aa9500-0355-431a-8815-effe7f5b1904",
   "metadata": {
    "execution": {
     "iopub.status.idle": "2024-09-12T14:07:56.911316Z",
     "shell.execute_reply": "2024-09-12T14:07:56.898223Z",
     "shell.execute_reply.started": "2024-09-12T14:07:48.594197Z"
    }
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'quantize_' from 'torchao.quantization.quant_api' (C:\\Users\\rajab\\miniconda3\\envs\\py3_120\\Lib\\site-packages\\torchao\\quantization\\quant_api.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchao\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquantization\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquant_api\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      2\u001b[0m     quantize_,\n\u001b[0;32m      3\u001b[0m     int8_dynamic_activation_int4_weight,\n\u001b[0;32m      4\u001b[0m     int8_dynamic_activation_int8_weight,\n\u001b[0;32m      5\u001b[0m     int8_dynamic_activation_int8_semi_sparse_weight,\n\u001b[0;32m      6\u001b[0m     int4_weight_only,\n\u001b[0;32m      7\u001b[0m     int8_weight_only\n\u001b[0;32m      8\u001b[0m )\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'quantize_' from 'torchao.quantization.quant_api' (C:\\Users\\rajab\\miniconda3\\envs\\py3_120\\Lib\\site-packages\\torchao\\quantization\\quant_api.py)"
     ]
    }
   ],
   "source": [
    "from torchao.quantization.quant_api import (\n",
    "    quantize_,\n",
    "    int8_dynamic_activation_int4_weight,\n",
    "    int8_dynamic_activation_int8_weight,\n",
    "    int8_dynamic_activation_int8_semi_sparse_weight,\n",
    "    int4_weight_only,\n",
    "    int8_weight_only\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd597c4d-3296-4e3d-b9b8-15eac55b4b06",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-13T09:19:40.409137Z",
     "iopub.status.busy": "2024-09-13T09:19:40.408499Z",
     "iopub.status.idle": "2024-09-13T09:19:40.646075Z",
     "shell.execute_reply": "2024-09-13T09:19:40.644974Z",
     "shell.execute_reply.started": "2024-09-13T09:19:40.409137Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
