{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "caddc2dd-257c-4129-bc35-13be901b720b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-15T08:38:58.764794Z",
     "iopub.status.busy": "2024-09-15T08:38:58.763286Z",
     "iopub.status.idle": "2024-09-15T08:39:08.962855Z",
     "shell.execute_reply": "2024-09-15T08:39:08.962855Z",
     "shell.execute_reply.started": "2024-09-15T08:38:58.764794Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import FluxPipeline\n",
    "import torch\n",
    "from torch.ao.quantization import quantize_dynamic\n",
    "import gc\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "class Quantizer:\n",
    "    def __init__(self):\n",
    "        self.type = None\n",
    "        self.dtype = torch.qint8\n",
    "        self.size_threshold_mb = 10\n",
    "    def set_model_name(self, name):\n",
    "        self.type = \"model\"\n",
    "        self.model_name = name\n",
    "    def set_pipeline(self, pipeline):\n",
    "        self.type = \"pipe\"\n",
    "        self.pipeline = pipeline\n",
    "        self.modules = self.get_all_modules_name(pipeline)\n",
    "    def quantize(self):\n",
    "        if self.type == \"pipe\":\n",
    "            for module in self.modules:\n",
    "                print(\"quantizing\", module)\n",
    "                self.quantize_pipeline(getattr(self.pipeline, module))\n",
    "        elif self.type == \"model\":\n",
    "            self.quantize_model()\n",
    "    def quantize_model(self,use_4bit=True, bnb_4bit_compute_dtype=\"float16\", \n",
    "                       bnb_4bit_quant_type= \"nf4\", use_nested_quant=False ):\n",
    "        compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=use_4bit,\n",
    "            bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "            bnb_4bit_compute_dtype=compute_dtype,\n",
    "            bnb_4bit_use_double_quant=use_nested_quant,\n",
    "        )\n",
    "        \n",
    "        # Check GPU compatibility with bfloat16\n",
    "        if compute_dtype == torch.float16 and use_4bit:\n",
    "            major, _ = torch.cuda.get_device_capability()\n",
    "            if major >= 8:\n",
    "                print(\"=\" * 80)\n",
    "                print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
    "                print(\"=\" * 80)\n",
    "                \n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.model_name,\n",
    "            quantization_config=bnb_config,\n",
    "        )\n",
    "    def get_all_modules_name(self, pipe):\n",
    "        res = []\n",
    "        for k in pipe.__dict__:\n",
    "            if isinstance(getattr(pipe, k), torch.nn.modules.module.Module):\n",
    "                res.append(k)\n",
    "        return res\n",
    "    def get_model_size(self, model):\n",
    "        param_size = 0\n",
    "        for param in model.parameters():\n",
    "            param_size += param.nelement() * param.element_size()\n",
    "        buffer_size = 0\n",
    "        for buffer in model.buffers():\n",
    "            buffer_size += buffer.nelement() * buffer.element_size()\n",
    "        size_all_mb = (param_size + buffer_size) / 1024**2\n",
    "        return size_all_mb\n",
    "    def quantize_module(self, module):\n",
    "        return quantize_dynamic(\n",
    "            module,\n",
    "            {torch.nn.Linear},\n",
    "            dtype=self.dtype\n",
    "        )\n",
    "    def quantize_pipeline(self, module, parent_name=''):\n",
    "        quantized = False\n",
    "        for name, submodule in module.named_children():\n",
    "            full_name = f\"{parent_name}.{name}\" if parent_name else name\n",
    "            size_mb = self.get_model_size(submodule)\n",
    "            \n",
    "            if self.size_threshold_mb is None or size_mb > self.size_threshold_mb:\n",
    "                print(f\"Quantizing {full_name} ({size_mb:.2f} MB)...\")\n",
    "                if list(submodule.children()):  # If submodule has children\n",
    "                    submodule = self.quantize_pipeline(submodule, full_name)\n",
    "                else:\n",
    "                    submodule = self.quantize_module(submodule)\n",
    "                setattr(module, name, submodule)\n",
    "                quantized = True\n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache()\n",
    "            else:\n",
    "                print(f\"Skipping {full_name} ({size_mb:.2f} MB) - below threshold\")\n",
    "        return module\n",
    "    def save(self, output_path):\n",
    "        if self.type == \"pipe\":\n",
    "            self.pipeline.save_pretrained(output_path)\n",
    "        elif self.type == \"model\":\n",
    "            self.model.save_pretrained(output_path)\n",
    "        else:\n",
    "            print(\"nothing to save\")\n",
    "class Main:\n",
    "    def quantize_pipeline(pipe):\n",
    "        q = Quantizer()\n",
    "        q.set_pipeline(pipe)\n",
    "        return q\n",
    "    def quantize_model(model_name):\n",
    "        q = Quantizer()\n",
    "        q.set_model_name(model_name)\n",
    "        return q\n",
    "\n",
    "# pipe = FluxPipeline.from_pretrained(\"black-forest-labs/FLUX.1-schnell\", torch_dtype=torch.bfloat16)\n",
    "# q = Quantizer()\n",
    "# q.size_threshold_mb = 150\n",
    "# q.set_pipeline(pipe)\n",
    "# print(q.modules)\n",
    "# q.quantize()\n",
    "# q.save(f\".models/flux-{q.size_threshold_mb}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "16152e2c-40d1-4129-b50b-2ade93c5b853",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T14:22:38.383848Z",
     "iopub.status.busy": "2024-09-11T14:22:38.383848Z",
     "iopub.status.idle": "2024-09-11T14:22:38.390562Z",
     "shell.execute_reply": "2024-09-11T14:22:38.389049Z",
     "shell.execute_reply.started": "2024-09-11T14:22:38.383848Z"
    }
   },
   "outputs": [],
   "source": [
    "* stable diffusion\n",
    "* "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
